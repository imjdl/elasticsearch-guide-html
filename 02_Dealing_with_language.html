<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.6.2">
<title>placeholder2</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Remove comment around @import statement below when using as a custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
[hidden],template{display:none}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}
input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*:before,*:after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.spread{width:100%}
p.lead,.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{font-size:1.21875em;line-height:1.6}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite:before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media only screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7;font-weight:bold}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix:before,.clearfix:after,.float-group:before,.float-group:after{content:" ";display:table}
.clearfix:after,.float-group:after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
*:not(pre)>code.nobreak{word-wrap:normal}
*:not(pre)>code.nowrap{white-space:nowrap}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button:before,b.button:after{position:relative;top:-1px;font-weight:400}
b.button:before{content:"[";padding:0 3px 0 2px}
b.button:after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header:before,#header:after,#content:before,#content:after,#footnotes:before,#footnotes:after,#footer:before,#footer:after{content:" ";display:table}
#header:after,#content:after,#footnotes:after,#footer:after{clear:both}
#content{margin-top:1.25em}
#content:before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span:before{content:"\00a0\2013\00a0"}
#header .details br+span.author:before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark:before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber:after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media only screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}
@media only screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
.sect1{padding-bottom:.625em}
@media only screen and (min-width:768px){.sect1{padding-bottom:1.25em}}
.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor:before,h2>a.anchor:before,h3>a.anchor:before,#toctitle>a.anchor:before,.sidebarblock>.content>.title>a.anchor:before,h4>a.anchor:before,h5>a.anchor:before,h6>a.anchor:before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock>caption.title{white-space:nowrap;overflow:visible;max-width:0}
.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>.paragraph:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:initial}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media only screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media only screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]:before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]:before{display:block}
.listingblock.terminal pre .command:before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt]):before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote:before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote:before{display:none}
.verseblock{margin:0 1em 1.25em 1em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 0 1.25em 0;display:block}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{text-align:left;word-spacing:0}
.quoteblock.abstract blockquote:before,.quoteblock.abstract blockquote p:first-of-type:before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
table.tableblock td>.paragraph:last-child p>p:last-child,table.tableblock th>p:last-child,table.tableblock td>p:last-child{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px 0}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot{border-width:1px 0}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{margin:0 auto .625em auto;margin-left:-1.375em;margin-right:0;padding:0;list-style:none;overflow:hidden}
ul.inline>li{list-style:none;float:left;margin-left:1.375em;display:block}
ul.inline>li>*{display:block}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist>table tr>td:first-of-type{padding:.4em .75em 0 .75em;line-height:1;vertical-align:top}
.colist>table tr>td:first-of-type img{max-width:initial}
.colist>table tr>td:last-of-type{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em 0;border-width:1px 0 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;text-indent:-1.05em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note:before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip:before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning:before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution:before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important:before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@media print{@page{margin:1.25cm .75cm}
*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare):after,a[href^="https:"]:not(.bare):after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]:after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
.sect1{padding-bottom:0!important}
.sect1+.sect1{border:0!important}
#header>h1:first-child{margin-top:1.25rem}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em 0}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span:before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]:before{display:block}
#footer{background:none!important;padding:0 .9375em}
#footer-text{color:rgba(0,0,0,.6)!important;font-size:.9em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
</style>
</head>
<body class="article">
<div id="header">
<h1>placeholder2</h1>
</div>
<div id="content">
<h1 id="languages" class="sect0">处理人类语言</h1>

<div class="sect1">
<h2 id="language-intro">开始处理各种语言</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Elasticsearch 为很多世界流行语言提供良好的、简单的、开箱即用的语言分析器集合：</p>
</div>
<div class="paragraph">
<p>阿拉伯语、亚美尼亚语、巴斯克语、巴西语、保加利亚语、加泰罗尼亚语、中文、捷克语、丹麦、荷兰语、英语、芬兰语、法语、加里西亚语、德语、希腊语、北印度语、匈牙利语、印度尼西亚、爱尔兰语、意大利语、日语、韩国语、库尔德语、挪威语、波斯语、葡萄牙语、罗马尼亚语、俄语、西班牙语、瑞典语、土耳其语和泰语。</p>
</div>
<div class="paragraph">
<p>这些分析器承担以下四种角色：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>文本拆分为单词：</p>
<div class="paragraph">
<p><code>The quick brown foxes</code> &#8594; [ <code>The</code>, <code>quick</code>, <code>brown</code>, <code>foxes</code>]</p>
</div>
</li>
<li>
<p>大写转小写：</p>
<div class="paragraph">
<p><code>The</code> &#8594; <code>the</code></p>
</div>
</li>
<li>
<p>移除常用的 <em>停用词</em>：</p>
<div class="paragraph">
<p>&#91; <code>The</code>, <code>quick</code>, <code>brown</code>, <code>foxes</code>] &#8594; [ <code>quick</code>, <code>brown</code>, <code>foxes</code>]</p>
</div>
</li>
<li>
<p>将变型词（例如复数词，过去式）转化为词根：</p>
<div class="paragraph">
<p><code>foxes</code> &#8594; <code>fox</code></p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>为了更好的搜索性，每个语言的分析器提供了该语言词汇的具体转换规则：</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>英语</code> 分析器移除了所有格 <code>'s</code></p>
<div class="paragraph">
<p><code>John&#8217;s</code> &#8594; <code>john</code></p>
</div>
</li>
<li>
<p><code>法语</code> 分析器移除了 <em>元音省略</em> 例如 <code>l'</code> 和 <code>qu'</code> 和 <em>变音符号</em> 例如 <code>¨</code> 或  <code>^</code> ：</p>
<div class="paragraph">
<p><code>l&#8217;église</code> &#8594; <code>eglis</code></p>
</div>
</li>
<li>
<p><code>德语</code> 分析器规范化了切词， 将切词中的 <code>ä</code> 和 <code>ae</code> 替换为 <code>a</code> ， 或将
<code>ß</code> 替换为 <code>ss</code> ：</p>
<div class="paragraph">
<p><code>äußerst</code> &#8594; <code>ausserst</code></p>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="using-language-analyzers">使用语言分析器</h3>
<div class="paragraph">
<p>Elasticsearch 的内置分析器都是全局可用的，不需要提前配置，它们也可以在字段映射中直接指定在某字段上：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "mappings": {
    "blog": {
      "properties": {
        "title": {
          "type":     "string",
          "analyzer": "english" <b class="conum">(1)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>title</code> 字段将会用 <code>english</code>（英语）分析器替换默认的 <code>standard</code>（标准）分析器</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>当然，文本经过 <code>english</code> 分析处理，我们会丢失源数据：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_analyze?field=title <b class="conum">(1)</b>
I'm not happy about the foxes</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>切词为: <code>i&#8217;m</code>，<code>happi</code>，<code>about</code>，<code>fox</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>我们无法分辨源文档中是包含单数 <code>fox</code> 还是复数 <code>foxes</code> ；单词 <code>not</code> 因为是停用词所以被移除了，
所以我们无法分辨源文档中是happy about foxes还是not happy about foxes，虽然通过使用 <code>english</code>
（英语）分析器，使得匹配规则更加宽松，我们也因此提高了召回率，但却降低了精准匹配文档的能力。</p>
</div>
<div class="paragraph">
<p>为了获得两方面的优势，我们可以使用<a href="#multi-fields">multifields</a>（多字段）对 <code>title</code> 字段建立两次索引：
一次使用 <code>english</code>（英语）分析器，另一次使用 <code>standard</code>（标准）分析器:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "mappings": {
    "blog": {
      "properties": {
        "title": { <b class="conum">(1)</b>
          "type": "string",
          "fields": {
            "english": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "english"
            }
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>主 <code>title</code> 字段使用 <code>standard</code>（标准）分析器。</p>
</li>
<li>
<p><code>title.english</code> 子字段使用 <code>english</code>（英语）分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>替换为该字段映射后，我们可以索引一些测试文档来展示怎么在搜索时使用两个字段：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/blog/1
{ "title": "I'm happy for this fox" }

PUT /my_index/blog/2
{ "title": "I'm not happy about my fox problem" }

GET /_search
{
  "query": {
    "multi_match": {
      "type":     "most_fields", <b class="conum">(1)</b>
      "query":    "not happy foxes",
      "fields": [ "title", "title.english" ]
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>使用<a href="#most-fields"><code>most_fields</code></a> query type（多字段搜索语法来）让我们可以用多个字段来匹配同一段文本。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>感谢 <code>title.english</code> 字段的切词，无论我们的文档中是否含有单词 <code>foxes</code> 都会被搜索到，第二份文档的相关性排行要比第一份高，
因为在 <code>title</code> 字段中匹配到了单词 <code>not</code> 。</p>
</div>
</div>
<div class="sect2">
<h3 id="configuring-language-analyzers">配置语言分析器</h3>
<div class="paragraph">
<p>语言分析器都不需要任何配置，开箱即用， 
它们中的大多数都允许你控制它们的各方面行为，具体来说：</p>
</div>
<div id="stem-exclusion" class="dlist">
<dl>
<dt class="hdlist1">词干提取排除</dt>
<dd>
<div class="paragraph">
<p>想象下某个场景，用户们想要搜索 <code>World Health Organization</code> 的结果，
但是却被替换为搜索 <code>organ health</code> 的结果。有这个困惑是因为 <code>organ</code> 和 <code>organization</code> 有相同的词根： <code>organ</code> 。
通常这不是什么问题，但是在一些特殊的文档中就会导致有歧义的结果，所以我们希望防止单词 <code>organization</code> 和 <code>organizations</code> 被缩减为词干。</p>
</div>
</dd>
<dt class="hdlist1">自定义停用词</dt>
<dd>
<p>英语中默认的停用词列表如下：</p>
<div class="literalblock">
<div class="content">
<pre>a, an, and, are, as, at, be, but, by, for, if, in, into, is, it,
no, not, of, on, or, such, that, the, their, then, there, these,
they, this, to, was, will, with</pre>
</div>
</div>
<div class="paragraph">
<p>关于单词 <code>no</code> 和 <code>not</code> 有点特别，这俩词会反转跟在它们后面的词汇的含义。或许我们应该认为这两个词很重要，不应该把他们看成停用词。</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>为了自定义 <code>english</code> （英语）分词器的行为，我们需要基于 <code>english</code> （英语）分析器创建一个自定义分析器，然后添加一些配置：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english": {
          "type": "english",
          "stem_exclusion": [ "organization", "organizations" ], <b class="conum">(1)</b>
          "stopwords": [ <b class="conum">(2)</b>
            "a", "an", "and", "are", "as", "at", "be", "but", "by", "for",
            "if", "in", "into", "is", "it", "of", "on", "or", "such", "that",
            "the", "their", "then", "there", "these", "they", "this", "to",
            "was", "will", "with"
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_english <b class="conum">(3)</b>
The World Health Organization does not sell organs.</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>防止 <code>organization</code> 和 <code>organizations</code> 被缩减为词干</p>
</li>
<li>
<p>指定一个自定义停用词列表</p>
</li>
<li>
<p>切词为 <code>world</code> 、 <code>health</code> 、 <code>organization</code> 、 <code>does</code> 、 <code>not</code> 、 <code>sell</code> 、 <code>organ</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>我们在 <a href="#stemming">将单词还原为词根</a> 和 <a href="#stopwords">停用词: 性能与精度</a> 中分别详细讨论了词干提取和停用词。</p>
</div>
</div>
<div class="sect2">
<h3 id="language-pitfalls">混合语言的陷阱</h3>
<div class="paragraph">
<p>如果你只需要处理一种语言，那么你很幸运。找到一个正确的策略用于处理多语言文档是一项巨大的挑战。</p>
</div>
<div class="sect3">
<h4 id="_在索引的时候">在索引的时候</h4>
<div class="paragraph">
<p>多语言文档主要有以下三个类型：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>一种是每份 <em>document</em> （文档）有自己的主语言，并包含一些其他语言的片段（参考 <a href="#one-lang-docs">每份文档一种语言</a>。）</p>
</li>
<li>
<p>一种是每个 <em>field</em> （域）有自己的主语言, 并包含一些其他语言的片段（参考 <a href="#one-lang-fields">每个域一种语言</a>。）</p>
</li>
<li>
<p>一种是每个 <em>field</em> （域）都是混合语言（参考 <a href="#mixed-lang-fields">混合语言域</a>。）</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>（分词）目标不总是可以实现，我们应当保持将不同语言分隔开。在同一份倒排索引内混合多种语言可能造成一些问题。</p>
</div>
<div class="sect4">
<h5 id="_不合理的词干提取">不合理的词干提取</h5>
<div class="paragraph">
<p>德语的词干提取规则跟英语，法语，瑞典语等是不一样的。  为不同的语言提供同样的词干提规则
将会导致有的词的词根找的正确，有的词的词根找的不正确，有的词根本找不到词根。 甚至是将不同语言的不同含义的词切为同一个词根，合并这些词根的搜索结果会给用户带来困恼。</p>
</div>
<div class="paragraph">
<p>提供多种的词干提取器轮流切分同一份文档的结果很有可能得到一堆垃圾，因为下一个词干提取器会尝试切分一个已经被缩减为词干的单词，这加剧了上面提到的问题。</p>
</div>
<div id="different-scripts" class="sidebarblock">
<div class="content">
<div class="title">每种书写方式一种词干提取器</div>
<div class="paragraph">
<p>只有一种情况, <em>only-one-stemmer</em> （唯一词干提取器）会发生，就是每种语言都有自己的书写方式。例如，在以色列就有很大的可能一个文档包含希伯来语，
阿拉伯语，俄语（古代斯拉夫语），和英语。</p>
</div>
<div class="literalblock">
<div class="content">
<pre>אזהרה - Предупреждение - تحذير - Warning</pre>
</div>
</div>
<div class="paragraph">
<p>每种语言使用不同的书写方式，所以一种语言的词干提取器就不会干扰其他语言的，允许为同一份文本提供多种词干提取器。</p>
</div>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_不正确的倒排文档频率">不正确的倒排文档频率</h5>
<div class="paragraph">
<p>在 <a href="#relevance-intro">[relevance-intro]</a> （相关性教程）中，一个 term （词）在一份文档中出现的频率约高，该term（词）的权重就越低。
为了精确的计算相关性，你需要精确的统计 term-frequency （词频）。</p>
</div>
<div class="paragraph">
<p>一段德文出现在英语为主的文本中会给与德语单词更高的权重，给那么高权重是因为德语单词相对来说更稀有。
但是如果这份文档跟以德语为主的文档混合在一起，那么这段德文就会有很低的权重。</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_在搜索的时候">在搜索的时候</h4>
<div class="paragraph">
<p>然而仅仅考虑你的文档是不够的  。你也需要考虑你的用户会怎么搜索这些文档。
通常你能从用户选择的语言界面来确定用户的主语言，（例如， <code>mysite.de</code> 和  <code>mysite.fr</code> ） 或者从用户的浏览器的HTTP header（HTTP头文件）
<a href="http://www.w3.org/International/questions/qa-lang-priorities.en.php"><code>accept-language</code></a> 确定。</p>
</div>
<div class="paragraph">
<p>用户的搜索也注意有三个方面:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>用户使用他的主语言搜索。</p>
</li>
<li>
<p>用户使用其他的语言搜索，但希望获取主语言的搜索结果。</p>
</li>
<li>
<p>用户使用其他语言搜索，并希望获取该语言的搜索结果。（例如，精通双语的人，或者网络咖啡馆的外国访问者）。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>根据你搜索数据的类型，或许会返回单语言的合适结果（例如，一个用户在西班牙网站搜索商品），也可能是用户主语言的搜索结果和其他语言的搜索结果混合。</p>
</div>
<div class="paragraph">
<p>通常来说，给与用户语言偏好的搜索很有意义。一个使用英语的用户搜索时更希望看到英语 Wikipedia 页面而不是法语 Wikipedia 页面。</p>
</div>
</div>
<div class="sect3">
<h4 id="identifying-language">语言识别</h4>
<div class="paragraph">
<p>你很可能已经知道你的文档所选用的语言，或者你的文档只是在你自己的组织内编写并被翻译成确定的一系列语言。人类的预识别可能是最可靠的将语言正确归类的方法。</p>
</div>
<div class="paragraph">
<p>然而，或许你的文档来自第三方资源且没经过语言归类，或者是不正确的归类。这种情况下，你需要一个学习算法来归类你文档的主语言。幸运的是，一些语言有现成的工具包可以帮你解决这个问题。</p>
</div>
<div class="paragraph">
<p>详细内容是来自
<a href="http://blog.mikemccandless.com/2013/08/a-new-version-of-compact-language.html">Mike McCandless</a> 的
<a href="https://github.com/mikemccand/chromium-compact-language-detector">chromium-compact-language-detector</a>
工具包，使用的是google开发的基于 (<a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>)的开源工具包
<a href="https://code.google.com/p/cld2/">Compact Language Detector</a> (CLD) 。
它小巧，快速，且精确，并能根据短短的两句话就可以检测 160+ 的语言。
它甚至能对单块文本检测多种语言。支持多种开发语言包括 Python，Perl，JavaScript，PHP，C#/.NET，和 R 。</p>
</div>
<div class="paragraph">
<p>确定用户搜索请求的语言并不是那么简单。 CLD 是为了至少 200 字符长的文本设计的。字符短的文本，例如搜索关键字，会产生不精确的结果。
这种情况下，或许采取一些简单的启发式算法会更好些，例如该国家的官方语言，用户选择的语言，和 HTTP <code>accept-language</code> headers （HTTP头文件）。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="one-lang-docs">每份文档一种语言</h3>
<div class="paragraph">
<p>每个主语言文档 只需要相当简单的设置。 不同语言的文档被分别存放在不同的索引中 &#x2014; <code>blogs-en</code> 、
<code>blogs-fr</code> ， 如此等等 &#x2014; 这样每个索引就可以使用相同的类型和相同的域，只是使用不同的分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /blogs-en
{
  "mappings": {
    "post": {
      "properties": {
        "title": {
          "type": "string", <b class="conum">(1)</b>
          "fields": {
            "stemmed": {
              "type":     "string",
              "analyzer": "english" <b class="conum">(2)</b>
            }
}}}}}}

PUT /blogs-fr
{
  "mappings": {
    "post": {
      "properties": {
        "title": {
          "type": "string", <b class="conum">(1)</b>
          "fields": {
            "stemmed": {
              "type":     "string",
              "analyzer": "french" <b class="conum">(2)</b>
            }
}}}}}}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>索引 <code>blogs-en</code> 和 <code>blogs-fr</code> 的 <code>post</code> 类型都有一个包含 <code>title</code> 域。</p>
</li>
<li>
<p><code>title.stemmed</code> 子域使用了具体语言的分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>这个方法干净且灵活。新语言很容易被添加&#8201;&#8212;&#8201;仅仅是创建一个新索引&#8212;&#8203;因为每种语言都是彻底的被分开，
我们不用遭受在 <a href="#language-pitfalls">混合语言的陷阱</a> 中描述的词频和词干提取的问题。</p>
</div>
<div class="paragraph">
<p>每一种语言的文档都可被独立查询，或者通过查询多种索引来查询多种语言。
我们甚至可以使用 <code>indices_boost</code> 参数为特定的语言添加优先权：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /blogs-*/post/_search <b class="conum">(1)</b>
{
    "query": {
        "multi_match": {
            "query":   "deja vu",
            "fields":  [ "title", "title.stemmed" ] <b class="conum">(2)</b>
            "type":    "most_fields"
        }
    },
    "indices_boost": { <b class="conum">(3)</b>
        "blogs-en": 3,
        "blogs-fr": 2
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>这个查询会在所有以 <code>blogs-</code> 开头的索引中执行。</p>
</li>
<li>
<p><code>title.stemmed</code> 字段使用每个索引中指定的分析器查询。</p>
</li>
<li>
<p>也许用户接受语言标头表明，更倾向于英语，然后是法语，所以相应的，我们会为每个索引的结果添加权重。任何其他语言会有一个中性的权重 1 。</p>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="_外语单词">外语单词</h4>
<div class="paragraph">
<p>当然，有些文档含有一些其他语言的单词或句子，且不幸的是这些单词被切为了正确的词根。对于主语言文档，这通常并不是主要的问题。用户经常需要搜索很精确的单词&#8212;&#8203;例如，一个其他语言的引用&#8212;&#8203;而不是语型变化过的单词。召回率 (Recall)可以通过使用 <a href="#token-normalization">归一化词元</a> 中讲解的技术提升。</p>
</div>
<div class="paragraph">
<p>假设有些单词例如地名应当能被主语言和原始语言都能检索，例如 <em>Munich</em> 和 <em>München</em> 。
这些单词实际上是我们在 <a href="#synonyms">同义词</a> 解释过的同义词。</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">不要对语言使用类型</div>
<div class="paragraph">
<p>你也许很倾向于为每个语言使用分开的类型，来代替使用分开的索引。
为了达到最佳效果，你应当避免使用类型。在 <a href="#mapping">[mapping]</a> 解释过，不同类型但有相同域名的域会被索引在 <em>相同的倒排索引</em> 中。这意味着不同类型（和不同语言）的词频混合在了一起。</p>
</div>
<div class="paragraph">
<p>为了确保一种语言的词频不会污染其他语言的词频，在后面的章节中会介绍到，无论是为每个语言使用单独的索引，还是使用单独的域都可以。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="one-lang-fields">每个域一种语言</h3>
<div class="paragraph">
<p>对于一些实体类，例如:产品、电影、法律声明，
通常这样的一份文本会被翻译成不同语言的文档。虽然这些不同语言的文档可以单独保存在各自的索引中。但另一种更合理的方式是同一份文本的所有翻译统一保存在一个索引中。。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">{
   "title":     "Fight club",
   "title_br":  "Clube de Luta",
   "title_cz":  "Klub rváčů",
   "title_en":  "Fight club",
   "title_es":  "El club de la lucha",
   ...
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>每份翻译存储在不同的域中，根据域的语言决定使用相应的分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /movies
{
  "mappings": {
    "movie": {
      "properties": {
        "title": { <b class="conum">(1)</b>
          "type":       "string"
        },
        "title_br": { <b class="conum">(2)</b>
            "type":     "string",
            "analyzer": "brazilian"
        },
        "title_cz": { <b class="conum">(2)</b>
            "type":     "string",
            "analyzer": "czech"
        },
        "title_en": { <b class="conum">(2)</b>
            "type":     "string",
            "analyzer": "english"
        },
        "title_es": { <b class="conum">(2)</b>
            "type":     "string",
            "analyzer": "spanish"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>title</code> 域含有title的原文，并使用 <code>standard</code> （标准）分析器。</p>
</li>
<li>
<p>其他字段使用适合自己语言的分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>在维持干净的词频方面，虽然 <em>index-per-language</em> （一种语言一份索引的方法），不像 <em>field-per-language</em> （一种语言一个域的方法）分开索引那么灵活。但是使用 <a href="#updating-a-mapping"><code>update-mapping</code> API</a> 添加一个新域也很简单，那些新域需要新的自定义分析器，这些新分析器只能在索引创建时被装配。有一个变通的方案，你可以先关闭这个索引  {ref}/indices-open-close.html[close] ，然后使用 {ref}/indices-update-settings.html[<code>update-settings</code> API] ，重新打开这个索引，但是关掉这个索引意味着得停止服务一段时间。</p>
</div>
<div class="paragraph">
<p>文档的一种语言可以单独查询，也可以通过查询多个域来查询多种语言。我们甚至可以通过对特定语言设置偏好来提高字段优先级：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /movies/movie/_search
{
    "query": {
        "multi_match": {
            "query":    "club de la lucha",
            "fields": [ "title*", "title_es^2" ], <b class="conum">(1)</b>
            "type":     "most_fields"
        }
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>这个搜索查询所有以 <code>title</code> 为前缀的域，但是对 <code>title_es</code> 域加权重 <code>2</code> 。其他的所有域是中性权重 <code>1</code> 。</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="mixed-lang-fields">混合语言域</h3>
<div class="paragraph">
<p>通常,那些从源数据中获得的多种语言混合在一个域中的文档会超出你的控制，
例如从网上爬取的页面：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">{ "body": "Page not found / Seite nicht gefunden / Page non trouvée" }</code></pre>
</div>
</div>
<div class="paragraph">
<p>正确的处理多语言类型文档是非常困难的。即使你简单对所有的域使用 <code>standard</code> （标准）分析器，
但你的文档会变得不利于搜索，除非你使用了合适的词干提取器。当然，你不可能只选择一个词干提取器。
词干提取器是由语言具体决定的。或者，词干提取器是由语言和脚本所具体决定的。像在 <a href="#different-scripts">每种书写方式一种词干提取器</a> 讨论中那样。
如果每个语言都使用不同的脚本，那么词干提取器就可以合并了。</p>
</div>
<div class="paragraph">
<p>假设你的混合语言使用的是一样的脚本，例如拉丁文，你有三个可用的选择：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>切分到不同的域</p>
</li>
<li>
<p>进行多次分析</p>
</li>
<li>
<p>使用 n-grams</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_切分到不同的域">切分到不同的域</h4>
<div class="paragraph">
<p>在 <a href="#identifying-language">语言识别</a> 提到过的紧凑的语言检测可以告诉你哪部分文档属于哪种语言。
你可以用 <a href="#one-lang-fields">每个域一种语言</a> 中用过的一样的方法来根据语言切分文本。</p>
</div>
</div>
<div class="sect3">
<h4 id="_进行多次分析">进行多次分析</h4>
<div class="paragraph">
<p>如果你主要处理数量有限的语言，
你可以使用多个域，每种语言都分析文本一次。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /movies
{
  "mappings": {
    "title": {
      "properties": {
        "title": { <b class="conum">(1)</b>
          "type": "string",
          "fields": {
            "de": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "german"
            },
            "en": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "english"
            },
            "fr": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "french"
            },
            "es": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "spanish"
            }
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>主域 <code>title</code> 使用 <code>standard</code> （标准）分析器</p>
</li>
<li>
<p>每个子域提供不同的语言分析器来对  <code>title</code> 域文本进行分析。</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_使用_n_grams">使用 n-grams</h4>
<div class="paragraph">
<p>你可以使用 <a href="#ngrams-compound-words">[ngrams-compound-words]</a> 中描述的方法索引所有的词汇为 n-grams。
大多数语型变化包含给单词添加一个后缀（或在一些语言中添加前缀），所以通过将单词拆成 n-grams，你有很大的机会匹配到相似但不完全一样的单词。
这个可以结合 <em>analyze-multiple times</em> （多次分析）方法为不支持的语言提供全域抓取：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /movies
{
  "settings": {
    "analysis": {...} <b class="conum">(1)</b>
  },
  "mappings": {
    "title": {
      "properties": {
        "title": {
          "type": "string",
          "fields": {
            "de": {
              "type":     "string",
              "analyzer": "german"
            },
            "en": {
              "type":     "string",
              "analyzer": "english"
            },
            "fr": {
              "type":     "string",
              "analyzer": "french"
            },
            "es": {
              "type":     "string",
              "analyzer": "spanish"
            },
            "general": { <b class="conum">(2)</b>
              "type":     "string",
              "analyzer": "trigrams"
            }
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>在 <code>analysis</code> 章节, 我们按照 <a href="#ngrams-compound-words">[ngrams-compound-words]</a> 中描述的定义了同样的 <code>trigrams</code> 分析器。</p>
</li>
<li>
<p>在 <code>title.general</code> 域使用 <code>trigrams</code> 分析器索引所有的语言。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>当查询抓取所有 <code>general</code> 域时，你可以使用 <code>minimum_should_match</code> （最少应当匹配数）来减少低质量的匹配。
或许也需要对其他字段进行稍微的加权，给与主语言域的权重要高于其他的在 <code>general</code> 上的域：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /movies/movie/_search
{
    "query": {
        "multi_match": {
            "query":    "club de la lucha",
            "fields": [ "title*^1.5", "title.general" ], <b class="conum">(1)</b>
            "type":     "most_fields",
            "minimum_should_match": "75%" <b class="conum">(2)</b>
        }
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>所有 <code>title</code> 或 <code>title.*</code> 域给与了比 <code>title.general</code> 域稍微高的加权。</p>
</li>
<li>
<p><code>minimum_should_match</code>（最少应当匹配数） 参数减少了低质量匹配的返回数, 这对 <code>title.general</code> 域尤其重要。</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="identifying-words">词汇识别</h2>
<div class="sectionbody">
<div class="paragraph">
<p>英语单词相对而言比较容易辨认：单词之间都是以空格或者（一些）标点隔开。
然而即使在英语词汇中也会有一些争议： <em>you&#8217;re</em> 是一个单词还是两个？ <em>o&#8217;clock</em> ， <em>cooperate</em> ， <em>half-baked</em> ，或者 <em>eyewitness</em> 这些呢？</p>
</div>
<div class="paragraph">
<p>德语或者荷兰语把独立的单词合并起来创造一个长的合成词如 <em>Weißkopfseeadler</em> (white-headed sea eagle) ,
但是为了在查询 <code>Adler</code> (eagle)的时候返回查询 <code>Weißkopfseeadler</code> 的结果，我们需要懂得怎么将合并词拆成词组。</p>
</div>
<div class="paragraph">
<p>亚洲的语言更复杂：很多语言在单词，句子，甚至段落之间没有空格。
有些词可以用一个字来表达，但是同样的字在另一个字旁边的时候就是不同意思的长词的一部分。</p>
</div>
<div class="paragraph">
<p>显而易见的是没有能够奇迹般处理所有人类语言的万能分析器，Elasticsearch 为很多语言提供了专用的分析器，
其他特殊语言的分析器以插件的形式提供。</p>
</div>
<div class="paragraph">
<p>然而并不是所有语言都有专用分析器，而且有时候你甚至无法确定处理的是什么语言。这种情况，我们需要一些忽略语言也能合理工作的标准工具包。</p>
</div>
<div class="sect2">
<h3 id="standard-analyzer">标准分析器</h3>
<div class="paragraph">
<p>任何全文检索的字符串域都默认使用 <code>standard</code> 分析器。
如果我们想要一个 <a href="#custom-analyzers"><code>自定义</code> 分析器</a> ，可以按照如下定义方式重新实现 <code>标准</code> 分析器：</p>
</div>
<div class="listingblock pagebreak-before">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">{
    "type":      "custom",
    "tokenizer": "standard",
    "filter":  [ "lowercase", "stop" ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>在 <a href="#token-normalization">归一化词元</a> （标准化词汇单元）和 <a href="#stopwords">停用词: 性能与精度</a> （停用词）中，我们讨论了 <code>lowercase</code> （小写字母）和 <code>stop</code> （停用词） <em>词汇单元过滤器</em> ，但是现在，我们专注于 <code>standard</code> <em>tokenizer</em> （标准分词器）。</p>
</div>
</div>
<div class="sect2">
<h3 id="standard-tokenizer">标准分词器</h3>
<div class="paragraph">
<p><em>分词器</em> 接受一个字符串作为输入，将这个字符串拆分成独立的词或 <em>语汇单元（token）</em>
（可能会丢弃一些标点符号等字符），然后输出一个 <em>语汇单元流（token stream）</em> 。</p>
</div>
<div class="paragraph">
<p>有趣的是用于词汇 <em>识别</em> 的算法。 <code>whitespace</code> （空白字符）分词器按空白字符 —— 空格、tabs、换行符等等进行简单拆分 —— 然后假定连续的非空格字符组成了一个语汇单元。例如：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=whitespace
You're the 1st runner home!</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个请求会返回如下词项（terms）：
<code>You&#8217;re</code> 、 <code>the</code> 、 <code>1st</code> 、 <code>runner</code> 、 <code>home!</code></p>
</div>
<div class="paragraph">
<p><code>letter</code> 分词器 ，采用另外一种策略，按照任何非字符进行拆分，
这样将会返回如下单词： <code>You</code> 、 <code>re</code> 、 <code>the</code> 、 <code>st</code> 、 <code>runner</code> 、 <code>home</code> 。</p>
</div>
<div class="paragraph">
<p><code>standard</code> 分词器使用 Unicode 文本分割算法
（定义来源于 <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>）来寻找单词 <em>之间</em> 的界限，并且输出所有界限之间的内容。
Unicode 内含的知识使其可以成功的对包含混合语言的文本进行分词。</p>
</div>
<div class="paragraph">
<p>标点符号可能是单词的一部分，也可能不是，这取决于它出现的位置：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard
You're my 'favorite'.</code></pre>
</div>
</div>
<div class="paragraph">
<p>在这个例子中，<code>You&#8217;re</code> 中的撇号被视为单词的一部分，然而 <code>'favorite'</code> 中的单引号则不会被视为单词的一部分，
所以分词结果如下： <code>You&#8217;re</code> 、 <code>my</code> 、 <code>favorite</code> 。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p><code>uax_url_email</code> 分词器和 <code>standard</code> 分词器工作方式极其相同。
区别只在于它能识别 email 地址和 URLs 并输出为单个语汇单元。
<code>standard</code> 分词器则不一样，会将 email 地址和 URLs 拆分成独立的单词。
例如，email 地址 <code>joe-bloggs@foo-bar.com</code> 的分词结果为 <code>joe</code> 、 <code>bloggs</code> 、 <code>foo</code> 、 <code>bar.com</code> 。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><code>standard</code> 分词器是大多数语言分词的一个合理的起点，特别是西方语言。
事实上，它构成了大多数特定语言分析器的基础，如 <code>english</code> 、<code>french</code> 和 <code>spanish</code> 分析器。
它也支持亚洲语言，只是有些缺陷，你可以考虑通过 ICU 插件的方式使用 <code>icu_tokenizer</code> 进行替换。</p>
</div>
</div>
<div class="sect2">
<h3 id="icu-plugin">安装 ICU 插件</h3>
<div class="paragraph">
<p>Elasticsearch的 <a href="https://github.com/elasticsearch/elasticsearch-analysis-icu">ICU 分析器插件</a> 使用 <em>国际化组件 Unicode</em> (ICU) 函数库（详情查看 <a href="http://site.icu-project.org">site.project.org</a> ）提供丰富的处理 Unicode 工具。
这些包含对处理亚洲语言特别有用的 <code>icu_分词器</code> ，还有大量对除英语外其他语言进行正确匹配和排序所必须的分词过滤器。</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>ICU 插件是处理英语之外语言的必需工具，非常推荐你安装并使用它，不幸的是，因为是基于额外的 ICU 函数库，
不同版本的ICU插件可能并不兼容之前的版本，当更新插件的时候，你需要重新索引你的数据。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>安装这个插件，第一步先关掉你的Elasticsearch节点，然后在Elasticsearch的主目录运行以下命令：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh">./bin/plugin -install elasticsearch/elasticsearch-analysis-icu/$VERSION <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>当前 <code>$VERSION</code> （版本）可以在以下地址找到
<em><a href="https://github.com/elasticsearch/elasticsearch-analysis-icu" class="bare">https://github.com/elasticsearch/elasticsearch-analysis-icu</a></em>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>一旦安装后，重启Elasticsearch，你将会看到类似如下的一条启动日志：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>[INFO][plugins] [Mysterio] loaded [marvel, analysis-icu], sites [marvel]</pre>
</div>
</div>
<div class="paragraph">
<p>如果你有很多节点并以集群方式运行的，你需要在集群的每个节点都安装这个插件。</p>
</div>
</div>
<div class="sect2">
<h3 id="icu-tokenizer">icu_分词器</h3>
<div class="paragraph">
<p><code>icu_分词器</code> 和 <code>标准分词器</code> 使用同样的 Unicode 文本分段算法，
只是为了更好的支持亚洲语，添加了泰语、老挝语、中文、日文、和韩文基于词典的词汇识别方法，并且可以使用自定义规则将缅甸语和柬埔寨语文本拆分成音节。</p>
</div>
<div class="paragraph">
<p>例如，分别比较 <code>标准分词器</code> 和 <code>icu_分词器</code> 在分词泰语中的 <code>'Hello. I am from Bangkok.'</code> 产生的词汇单元：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard
สวัสดี ผมมาจากกรุงเทพฯ</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>标准分词器</code> 产生了两个词汇单元，每个句子一个： <code>สวัสดี</code> ， <code>ผมมาจากกรุงเทพฯ</code> 。这个只是你想搜索整个句子 <code>'I am from Bangkok.'</code> 的时候有用，但是如果你仅想搜索 <code>'Bangkok.'</code> 则不行。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=icu_tokenizer
สวัสดี ผมมาจากกรุงเทพฯ</code></pre>
</div>
</div>
<div class="paragraph">
<p>相反， <code>icu_分词器</code> 可以把文本分成独立的单词（ <code>สวัสดี</code> ， <code>ผม</code> ， <code>มา</code> ， <code>จาก</code> ， <code>กรุงเทพฯ</code> ），这使得文档更容易被搜索到。</p>
</div>
<div class="paragraph">
<p>相较而言, <code>标准分词器</code> 分词中文和日文的时候“过度分词”了，经常将一个完整的词拆分为独立的字符，因为单词之间并没有空格，很难区分连续的字符是间隔的单词还是一个句子中的单字：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>向的意思是 <em>facing</em> （面对）， 日的意思是 <em>sun</em> （太阳），葵的意思是 <em>hollyhock</em> （蜀葵）。当写在一起的时候, 向日葵的意思是 <em>sunflower</em> （向日葵）。</p>
</li>
<li>
<p>五的意思是 <em>five</em> （五）或者  <em>fifth</em> （第五）， 月的意思是 <em>month</em> （月份），雨的意思是 <em>rain</em> （下雨）。
第一个和第二个字符写在一起成了五月，意思是 <em>the month of May</em>（一年中的五月）， 然而添加上第三个字符, 五月雨的意思是
<em>continuous rain</em> （连续不断的下雨,梅雨）。当在合并第四个字符， 式，
意思是 <em>style</em> （样式），五月雨式这个单词则成了一种不屈不挠持续不断的东西的形容词。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>虽然每个字符本身可以是一个单词，但使词汇单元保持更大的原始概念比使其仅作为一个词组的一部分要有意义的多：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard
向日葵

GET /_analyze?tokenizer=icu_tokenizer
向日葵</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>标准分词器</code> 在前面的例子中将每个字符输出为单独的词汇单元： <code>向</code> ， <code>日</code> ， <code>葵</code> 。 <code>icu_分词器</code> 会输出单个词汇单元 <code>向日葵</code> （sunflower） 。</p>
</div>
<div class="paragraph">
<p><code>标准分词器</code> 和 <code>icu_分词器</code> 的另一个不同的地方是后者会将不同书写方式的字符（例如，<code>βeta</code> ）拆分成独立的词汇单元 &#x2014; <code>β</code> 和 <code>eta</code>&#x2014; ，而前者则会输出单个词汇单元： <code>βeta</code> 。</p>
</div>
</div>
<div class="sect2">
<h3 id="char-filters">整理输入文本</h3>
<div class="paragraph">
<p>当输入文本是干净的时候分词器提供最佳分词结果，有效文本，这里 <em>有效</em> 指的是遵从 Unicode 算法期望的标点符号规则。
然而很多时候，我们需要处理的文本会是除了干净文本之外的任何文本。在分词之前整理文本会提升输出结果的质量。</p>
</div>
<div class="sect3">
<h4 id="_html_分词">HTML 分词</h4>
<div class="paragraph">
<p>将 HTML 通过 <code>标准分词器</code> 或 <code>icu_分词器</code> 分词将产生糟糕的结果。这些分词器不知道如何处理 HTML 标签。例如：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard
&lt;p&gt;Some d&amp;eacute;j&amp;agrave; vu &lt;a href="http://somedomain.com&gt;"&gt;website&lt;/a&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>标准分词器</code> 会混淆 HTML 标签和实体，并且输出以下词汇单元： <code>p</code> 、 <code>Some</code> 、 <code>d</code> 、 <code>eacute</code> 、 <code>j</code> 、 <code>agrave</code> 、 <code>vu</code> 、 <code>a</code> 、
<code>href</code> 、 <code>http</code> 、 <code>somedomain.com</code> 、 <code>website</code> 、 <code>a</code> 。这些词汇单元显然不知所云！</p>
</div>
<div class="paragraph">
<p><em>字符过滤器</em> 可以添加进分析器中，在将文本传给分词器之前预处理该文本。在这种情况下，我们可以用 <code>html_strip</code> 字符过滤器移除 HTML 标签并编码 HTML 实体如 <code>&eacute;</code> 为一致的 Unicode 字符。</p>
</div>
<div class="paragraph">
<p>字符过滤器可以通过 <code>analyze</code> API 进行测试，这需要在查询字符串中指明它们：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard&amp;char_filters=html_strip
&lt;p&gt;Some d&amp;eacute;j&amp;agrave; vu &lt;a href="http://somedomain.com&gt;"&gt;website&lt;/a&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>想将它们作为分析器的一部分使用，需要把它们添加到 <code>custom</code> 类型的自定义分析器里：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_html_analyzer": {
                    "tokenizer":     "standard",
                    "char_filter": [ "html_strip" ]
                }
            }
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>一旦自定义分析器创建好之后， 我们新的 <code>my_html_analyzer</code> 就可以用 <code>analyze</code> API 测试：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_analyze?analyzer=my_html_analyzer
&lt;p&gt;Some d&amp;eacute;j&amp;agrave; vu &lt;a href="http://somedomain.com&gt;"&gt;website&lt;/a&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>这次输出的词汇单元才是我们期望的： <code>Some</code> ， déjà ， <code>vu</code> ， <code>website</code> 。</p>
</div>
</div>
<div class="sect3">
<h4 id="_整理标点符号">整理标点符号</h4>
<div class="paragraph">
<p><code>标准分词器</code> 和 <code>icu_分词器</code> 都能理解单词中的撇号应当被视为单词的一部分，然而包围单词的单引号在不应该。分词文本  <code>You&#8217;re my 'favorite'</code> ，
会被输出正确的词汇单元 <code>You&#8217;re ， my ， favorite</code> 。</p>
</div>
<div class="paragraph">
<p>不幸的是， Unicode 列出了一些有时会被用为撇号的字符：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>U+0027</code></dt>
<dd>
<p>撇号标记为 (<code>'</code>)&#x2014; 原始 ASCII 符号</p>
</dd>
<dt class="hdlist1"><code>U+2018</code></dt>
<dd>
<p>左单引号标记为 (<code>‘</code>)&#x2014; 当单引用时作为一个引用的开始</p>
</dd>
<dt class="hdlist1"><code>U+2019</code></dt>
<dd>
<p>右单引号标记为 (<code>’</code>)&#x2014; 当单引用时座位一个引用的结束，也是撇号的首选字符。</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>当这三个字符出现在单词中间的时候， <code>标准分词器</code> 和 <code>icu_分词器</code> 都会将这三个字符视为撇号（这会被视为单词的一部分）。
然而还有另外三个长得很像撇号的字符：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>U+201B</code></dt>
<dd>
<p>Single high-reversed-9 （高反单引号）标记为  (<code>‛</code>)&#x2014; 跟 <code>U+2018</code> 一样，但是外观上有区别</p>
</dd>
<dt class="hdlist1"><code>U+0091</code></dt>
<dd>
<p>ISO-8859-1 中的左单引号 &#x2014; 不会被用于 Unicode 中</p>
</dd>
<dt class="hdlist1"><code>U+0092</code></dt>
<dd>
<p>ISO-8859-1 中的右单引号 &#x2014; 不会被用于 Unicode 中</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p><code>标准分词器</code> 和 <code>icu_分词器</code> 把这三个字符视为单词的分界线&#8201;&#8212;&#8201;一个将文本拆分为词汇单元的位置。不幸的是，一些出版社用 <code>U+201B</code> 作为名字的典型书写方式例如 <code>M‛coy</code> ，
第二个俩字符或许可以被你的文字处理软件打出来，这取决于这款软件的年纪。</p>
</div>
<div class="paragraph">
<p>即使在使用可以“接受”的引号标记时，一个用单引号书写的词 &#x2014; <code>You’re</code> &#x2014; 也和一个用撇号书写的词 &#x2014; <code>You&#8217;re</code> &#x2014; 不一样，这意味着搜索其中的一个变体将会找不到另一个。</p>
</div>
<div class="paragraph">
<p>幸运的是，可以用 <code>mapping</code> 对这些混乱的字符进行分类，
该过滤器可以运行我们用另一个字符替换所有实例中的一个字符。这种情况下，我们可以简单的用 <code>U+0027</code> 替换所有的撇号变体：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": { <b class="conum">(1)</b>
        "quotes": {
          "type": "mapping",
          "mappings": [ <b class="conum">(2)</b>
            "\\u0091=&gt;\\u0027",
            "\\u0092=&gt;\\u0027",
            "\\u2018=&gt;\\u0027",
            "\\u2019=&gt;\\u0027",
            "\\u201B=&gt;\\u0027"
          ]
        }
      },
      "analyzer": {
        "quotes_analyzer": {
          "tokenizer":     "standard",
          "char_filter": [ "quotes" ] <b class="conum">(3)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>我们自定义了一个 <code>char_filter</code> （字符过滤器）叫做 <code>quotes</code> ，提供所有撇号变体到简单撇号的映射。</p>
</li>
<li>
<p>为了更清晰，我们使用每个字符的 JSON Unicode 转义语句，当然我们也可以使用他们本身字符表示： <code>"‘&#8658;'"</code> 。</p>
</li>
<li>
<p>我们用自定义的 <code>quotes</code> 字符过滤器创建一个新的分析器叫做 <code>quotes_analyzer</code> 。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>像以前一样，我们需要在创建了分析器后测试它：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_analyze?analyzer=quotes_analyzer
You’re my ‘favorite’ M‛Coy</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个例子返回如下词汇单元，其中所有的单词中的引号标记都被替换为了撇号： <code>You&#8217;re</code>, <code>my</code>, <code>favorite</code>, <code>M&#8217;Coy</code> 。</p>
</div>
<div class="paragraph">
<p>投入更多的努力确保你的分词器接收到高质量的输入，你的搜索结果质量也将会更好。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="token-normalization">归一化词元</h2>
<div class="sectionbody">
<div class="paragraph">
<p>把文本切割成词元(token)只是这项工作的一半。为了让这些词元(token)更容易搜索, 这些词元(token)需要被 <em>归一化</em>(<em>normalization</em>)--这个过程会去除同一个词元(token)的无意义差别，例如大写和小写的差别。可能我们还需要去掉有意义的差别, 让 <code>esta</code>、<code>ésta</code> 和 <code>está</code> 都能用同一个词元(token)来搜索。你会用 <code>déjà vu</code> 来搜索，还是 <code>deja vu</code>?</p>
</div>
<div class="paragraph">
<p>这些都是语汇单元过滤器的工作。语汇单元过滤器接收来自分词器(tokenizer)的词元(token)流。还可以一起使用多个语汇单元过滤器，每一个都有自己特定的处理工作。每一个语汇单元过滤器都可以处理来自另一个语汇单元过滤器输出的单词流。</p>
</div>
<div class="sect2">
<h3 id="lowercase-token-filter">举个例子</h3>
<div class="paragraph">
<p>用的最多的语汇单元过滤器(token filters)是 <code>lowercase</code> 过滤器，它的功能正和你期望的一样；它将每个词元(token)转换为小写形式：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?tokenizer=standard&amp;filters=lowercase
The QUICK Brown FOX! <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>得到的词元(token)是 <code>the</code>, <code>quick</code>, <code>brown</code>, <code>fox</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>只要查询和检索的分析过程是一样的，不管用户搜索 <code>fox</code> 还是 <code>FOX</code> 都能得到一样的搜索结果。<code>lowercase</code> 过滤器会将查询 <code>FOX</code> 的请求转换为查询 <code>fox</code> 的请求， <code>fox</code> 和我们在倒排索引中存储的是同一个词元(token)。</p>
</div>
<div class="paragraph">
<p>为了在分析过程中使用 token 过滤器，我们可以创建一个 <code>custom</code> 分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_lowercaser": {
          "tokenizer": "standard",
          "filter":  [ "lowercase" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>我们可以通过 <code>analyze</code> API 来验证:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_analyze?analyzer=my_lowercaser
The QUICK Brown FOX! <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>得到的词元是 <code>the</code>, <code>quick</code>, <code>brown</code>, <code>fox</code></p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="asciifolding-token-filter">如果有口音</h3>
<div class="paragraph">
<p>英语用变音符号(例如 <code>´</code>, <code>^</code>, 和 <code>¨</code>) 来强调单词—​例如 <code>rôle</code>, déjà, 和 <code>däis</code> —​但是是否使用他们通常是可选的.  其他语言则通过变音符号来区分单词。当然，只是因为在你的索引中拼写正确的单词并不意味着用户将搜索正确的拼写。
去掉变音符号通常是有用的，让 <code>rôle</code> 对应 <code>role</code>, 或者反过来。 对于西方语言，可以用 <code>asciifolding</code> 字符过滤器来实现这个功能。  实际上，它不仅仅能去掉变音符号。它会把Unicode字符转化为ASCII来表示:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ß</code> &#8658; <code>ss</code></p>
</li>
<li>
<p><code>æ</code> &#8658; <code>ae</code></p>
</li>
<li>
<p><code>ł</code> &#8658; <code>l</code></p>
</li>
<li>
<p><code>ɰ</code> &#8658; <code>m</code></p>
</li>
<li>
<p><code>⁇</code> &#8658; <code>??</code></p>
</li>
<li>
<p><code>❷</code> &#8658; <code>2</code></p>
</li>
<li>
<p><code>⁶</code> &#8658; <code>6</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>像 <code>lowercase</code> 过滤器一样,  <code>asciifolding</code> 不需要任何配置，可以被 <code>custom</code> 分析器直接使用:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "folding": {
          "tokenizer": "standard",
          "filter":  [ "lowercase", "asciifolding" ]
        }
      }
    }
  }
}

GET /my_index?analyzer=folding
My œsophagus caused a débâcle <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>得到的词元 <code>my</code>, <code>oesophagus</code>, <code>caused</code>, <code>a</code>, <code>debacle</code></p>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="_保留原意">保留原意</h4>
<div class="paragraph">
<p>理所当然的，去掉变音符号会丢失原意。
例如, 参考 这三个 西班牙单词:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>esta</code></dt>
<dd>
<p>形容词 <em>this</em> 的阴性形式, 例如 <em>esta silla</em> (this chair) 和 <em>esta</em> (this one).</p>
</dd>
<dt class="hdlist1"><code>ésta</code></dt>
<dd>
<p><code>esta</code> 的古代用法.</p>
</dd>
<dt class="hdlist1"><code>está</code></dt>
<dd>
<p>动词 <em>estar</em> (to be) 的第三人称形式, 例如 <em>está feliz</em> (he is happy).</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>通常我们会合并前两个形式的单词，而去区分和他们不相同的第三个形式的单词。类似的:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>sé</code></dt>
<dd>
<p>动词 <em>saber</em> (to know) 的第一人称形式 例如 <em>Yo sé</em>  (I know).</p>
</dd>
<dt class="hdlist1"><code>se</code></dt>
<dd>
<p>与许多动词使用的第三人称反身代词, 例如 <em>se sabe</em> (it is known).</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>不幸的是，没有简单的方法，去区分哪些词应该保留变音符号和哪些词应该去掉变音符号。而且很有可能，你的用户也不知道.</p>
</div>
<div class="paragraph">
<p>相反， 我们对文本做两次索引: 一次用原文形式，一次用去掉变音符号的形式:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/_mapping/my_type
{
  "properties": {
    "title": { <b class="conum">(1)</b>
      "type":           "string",
      "analyzer":       "standard",
      "fields": {
        "folded": { <b class="conum">(2)</b>
          "type":       "string",
          "analyzer":   "folding"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>在 <code>title</code> 字段用 <code>standard</code> 分析器，会保留原文的变音符号.</p>
</li>
<li>
<p>在 <code>title.folded</code> 字段用 <code>folding</code> 分析器，会去掉变音符号</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>你可以使用 <code>analyze</code> API 分析 <em>Esta está loca</em> (This woman is crazy)这个句子，来验证字段映射:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_analyze?field=title <b class="conum">(1)</b>
Esta está loca

GET /my_index/_analyze?field=title.folded <b class="conum">(2)</b>
Esta está loca</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>得到的词元 <code>esta</code>, <code>está</code>, <code>loca</code></p>
</li>
<li>
<p>得到的词元 <code>esta</code>, <code>esta</code>, <code>loca</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>可以用更多的文档来测试:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/my_type/1
{ "title": "Esta loca!" }

PUT /my_index/my_type/2
{ "title": "Está loca!" }</code></pre>
</div>
</div>
<div class="paragraph">
<p>现在，我们可以通过联合所有的字段来搜索。在`multi_match`查询中通过 <a href="#most-fields"><code>most_fields</code> mode</a> 模式来联合所有字段的结果:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_search
{
  "query": {
    "multi_match": {
      "type":     "most_fields",
      "query":    "esta loca",
      "fields": [ "title", "title.folded" ]
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>通过 <code>validate-query</code> API 来执行这个查询可以帮助你理解查询是如何执行的:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /my_index/_validate/query?explain
{
  "query": {
    "multi_match": {
      "type":     "most_fields",
      "query":    "está loca",
      "fields": [ "title", "title.folded" ]
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>multi-match</code> 查询会搜索在 <code>title</code> 字段中原文形式的单词 (<code>está</code>)，和在 <code>title.folded</code> 字段中去掉变音符号形式的单词 <code>esta</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>(title:está        title:loca       )
(title.folded:esta title.folded:loca)</pre>
</div>
</div>
<div class="paragraph">
<p>无论用户搜索的是 <code>esta</code> 还是 <code>está</code>; 两个文档都会被匹配，因为去掉变音符号形式的单词在
<code>title.folded</code> 字段中。然而，只有原文形式的单词在 <code>title</code> 字段中。此额外匹配会把包含原文形式单词的文档排在结果列表前面。</p>
</div>
<div class="paragraph">
<p>我们用 <code>title.folded</code> 字段来 <em>扩大我们的网</em> (<em>widen the net</em>)来匹配更多的文档，然后用原文形式的 <code>title</code> 字段来把关联度最高的文档排在最前面。在可以为了匹配数量牺牲文本原意的情况下，这个技术可以被用在任何分析器里。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p><code>asciifolding</code> 过滤器有一个叫做 <code>preserve_original</code> 的选项可以让你这样来做索引，把词的原文词元(original token)和处理&#8212;&#8203;折叠后的词元(folded token)放在同一个字段的同一个位置。开启了这个选项，结果会像这样:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Position 1     Position 2
--------------------------
(ésta,esta)    loca
--------------------------</pre>
</div>
</div>
<div class="paragraph">
<p>虽然这个是节约空间的好办法，但是也意味着没有办法再说“给我精确匹配的原文词元”(Give me an exact match on the original word)。包含去掉和不去掉变音符号的词元，会导致不可靠的相关性评分。</p>
</div>
<div class="paragraph">
<p>所以，正如我们这一章做的，把每个字段的不同形式分开到不同的字段会让索引更清晰。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="unicode-normalization">Unicode的世界</h3>
<div class="paragraph">
<p>当Elasticsearch在比较词元(token)的时候，它是进行字节(byte)级别的比较。 换句话说，如果两个词元(token)被判定为相同的话，他们必须是相同的字节(byte)组成的。然而，Unicode允许你用不同的字节来写相同的字符。</p>
</div>
<div class="paragraph">
<p>例如， <em>&#x00e9;</em> 和 <em>e&#769;</em> 的不同是什么？这取决于你问谁。对于Elasticsearch，第一个是由 <code>0xC3 0xA9</code> 这两个字节组成的，第二个是由 <code>0x65
0xCC 0x81</code> 这三个字节组成的。</p>
</div>
<div class="paragraph">
<p>对于Unicode，他们的差异和他们的怎么组成没有关系，所以他们是相同的。第一个是单个单词 <code>é</code> ，第二个是一个简单 <code>e</code> 和重音符 ´。</p>
</div>
<div class="paragraph">
<p>如果你的数据有多个来源，就会有可能发生这种状况：因为相同的单词使用了不同的编码，导致一个形式的 déjà 不能和它的其他形式进行匹配。</p>
</div>
<div class="paragraph">
<p>幸运的是，这里就有解决办法。这里有4种Unicode <em>归一化形式</em> (<em>normalization forms</em>) : <code>nfc</code>, <code>nfd</code>, <code>nfkc</code>, <code>nfkd</code>，它们都把Unicode字符转换成对应标准格式，把所有的字符 进行字节(byte)级别的比较。</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">Unicode归一化形式 (Normalization Forms)</div>
<div class="literalblock">
<div class="content">
<pre>_组合_ (_composed_) 模式—`nfc` 和 `nfkc`—用尽可能少的字节(byte)来代表字符。 ((("composed forms (Unicode normalization)"))) 所以用 `é` 来代表单个字母 `é` 。  _分解_ （_decomposed_） 模式—`nfd` and `nfkd`—用字符的每一部分来代表字符。所以 `é` 分解为 `e` 和 `´`。 ((("decomposed forms (Unicode normalization)")))</pre>
</div>
</div>
<div class="paragraph">
<p><em>规范</em> (<em>canonical</em>) 模式—<code>nfc</code> 和 <code>nfd</code>&amp;—把连字作为单个字符，例如 <code>ﬃ</code> 或者 <code>œ</code> 。 <em>兼容</em> (<em>compatibility</em>) 模式—<code>nfkc</code> 和
<code>nfkd</code>—将这些组合的字符分解成简单字符的等价物，例如： <code>f</code> + <code>f</code> + <code>i</code> 或者 <code>o</code> + <code>e</code>.</p>
</div>
</div>
</div>
<div class="paragraph">
<p>无论你选择哪一个归一化(normalization)模式，只要你的文本只用一种模式，那你的同一个词元(token)就会由相同的字节(byte)组成。例如，<em>兼容</em> (<em>compatibility</em>) 模式  可以用连词 <code>ﬃ</code> 的简化形式 `ffi`来进行对比。</p>
</div>
<div class="paragraph">
<p>你可以使用 <code>icu_normalizer</code> 语汇单元过滤器(token filters)  来保证你的所有词元(token)是相同模式：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "nfkc_normalizer": { <b class="conum">(1)</b>
          "type": "icu_normalizer",
          "name": "nfkc"
        }
      },
      "analyzer": {
        "my_normalizer": {
          "tokenizer": "icu_tokenizer",
          "filter":  [ "nfkc_normalizer" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>用 <code>nfkc</code> 归一化(normalization)模式来归一化(Normalize)所有词元(token).</p>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>包括刚才提到过的 <code>icu_normalizer</code> 语汇单元过滤器(token filters)在内，这里还有 <code>icu_normalizer</code>  <em>字符</em> 过滤器(<em>character</em> filters)。虽然它和语汇单元过滤器做相同的工作，但是会在文本到达过滤器之前做。到底是用`standard` 过滤器，还是 <code>icu_tokenizer</code> 过滤器，其实并不重要。因为过滤器知道怎么来正确处理所有的模式。</p>
</div>
<div class="paragraph">
<p>但是，如果你使用不同的分词器，例如： <code>ngram</code>, <code>edge_ngram</code>, 或者 <code>pattern</code> 分词器，那么在语汇单元过滤器(token filters)之前使用 <code>icu_normalizer</code>  字符过滤器就变得有意义了。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>通常来说，你不仅仅想要归一化(normalize)词元(token)的字节(byte)规则，还需要把他们转成小写字母。这个可以通过 <code>icu_normalizer</code> 和定制的归一化(normalization)的模式 <code>nfkc_cf</code> 来实现。下一节我们会具体讲这个。</p>
</div>
</div>
<div class="sect2">
<h3 id="case-folding">Unicode 大小写折叠</h3>
<div class="paragraph">
<p>人类没有创造力的话就不会是人类， 而人类的语言就恰恰反映了这一点。</p>
</div>
<div class="paragraph">
<p>处理一个单词的大小写看起来是一个简单的任务，除非遇到需要处理多语言的情况。</p>
</div>
<div class="paragraph">
<p>那就举一个例子：转换小写德国单词 <code>ß</code>。把它转换成大写是 <code>SS</code>，然后在转换成小写就成了 <code>ss</code>。还有一个例子：转换希腊字母 <code>ς</code> (sigma, 在单词末尾使用)。把它转换成大写是 <code>Σ</code>，然后再转换成小写就成了 <code>σ</code>。</p>
</div>
<div class="paragraph">
<p>把词条小写的核心是让他们看起来更像，而不是更不像。在Unicode中，这个工作是大小写折叠(case folding)来完成的，而不是小写化(lowercasing)。  <em>大小写折叠</em> (<em>Case folding</em>) 把单词转换到一种(通常是小写)形式，是让写法不会影响单词的比较，所以拼写不需要完全正确。</p>
</div>
<div class="paragraph">
<p>例如：单词 <code>ß</code>，已经是小写形式了，会被_折叠_(<em>folded</em>)成 <code>ss</code>。类似的小写的 <code>ς</code> 被折叠成 <code>σ</code>，这样的话，无论 <code>σ</code>， <code>ς</code>， 和 `Σ`出现在哪里， 他们就都可以比较了。</p>
</div>
<div class="literalblock">
<div class="content">
<pre>`icu_normalizer` 语汇单元过滤器默认的归一化(normalization)模式是 `nfkc_cf`。它像 `nfkc` 模式一样：</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><em>组合</em> (<em>Composes</em>) 字符用最短的字节来表示。</p>
</li>
<li>
<p>用 <em>兼容</em> (<em>compatibility</em>）模式，把像 <code>ﬃ</code> 的字符转换成简单的 <code>ffi</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>但是，也会这样做：</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>大小写折叠</em> (<em>Case-folds</em>) 字符成一种适合比较的形式</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>换句话说， <code>nfkc_cf`等价于 `lowercase</code> 语汇单元过滤器(token filters)，但是却适用于所有的语言。 <em>on-steroids</em> 等价于 <code>standard</code> 分析器，例如：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_lowercaser": {
          "tokenizer": "icu_tokenizer",
          "filter":  [ "icu_normalizer" ] <b class="conum">(1)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>icu_normalizer</code> 默认是 <code>nfkc_cf</code> 模式.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>我们来比较 <code>Weißkopfseeadler`和 `WEISSKOPFSEEADLER</code>(大写形式) 分别通过 `standard`分析器和我们的Unicode自识别(Unicode-aware)分析器处理得到的结果：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">GET /_analyze?analyzer=standard <b class="conum">(1)</b>
Weißkopfseeadler WEISSKOPFSEEADLER

GET /my_index/_analyze?analyzer=my_lowercaser <b class="conum">(2)</b>
Weißkopfseeadler WEISSKOPFSEEADLER</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>得到的词元(token)是 <code>weißkopfseeadler</code>, <code>weisskopfseeadler</code></p>
</li>
<li>
<p>得到的词元(token)是 <code>weisskopfseeadler</code>, <code>weisskopfseeadler</code></p>
<div class="literalblock">
<div class="content">
<pre>`standard`分析器得到了两个不同且不可比较的词元(token)，而我们定制化的分析器得到了两个相同但是不符合原意的词元(token)。</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="character-folding">Unicode 字符折叠</h3>
<div class="literalblock">
<div class="content">
<pre>在多语言((("Unicode", "character folding")))((("tokens", "normalizing", "Unicode character folding")))处理中，`lowercase` 语汇单元过滤器(token filters)是一个很好的开始。但是作为对比的话，也只是对于整个巴别塔的惊鸿一瞥。所以 &lt;&lt;asciifolding-token-filter,`asciifolding` token filter&gt;&gt; 需要更有效的Unicode _字符折叠_ (_character-folding_)工具来处理全世界的各种语言。((("asciifolding token filter")))</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>`icu_folding` 语汇单元过滤器(token filters) (provided by the &lt;&lt;icu-plugin,`icu` plug-in&gt;&gt;)的功能和 `asciifolding` 过滤器一样， ((("icu_folding token filter")))但是它扩展到了非ASCII编码的语言，例如：希腊语，希伯来语，汉语。它把这些语言都转换对应拉丁文字，甚至包含它们的各种各样的计数符号，象形符号和标点符号。</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>`icu_folding` 语汇单元过滤器(token filters)自动使用 `nfkc_cf` 模式来进行大小写折叠和Unicode归一化(normalization)，所以不需要使用 `icu_normalizer` ：</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_folder": {
          "tokenizer": "icu_tokenizer",
          "filter":  [ "icu_folding" ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_folder
١٢٣٤٥ <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>阿拉伯数字 <code>١٢٣٤٥</code> 被折叠成等价的拉丁数字: <code>12345</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>如果你有指定的字符不想被折叠，你可以使用 <a href="http://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html"><em>UnicodeSet</em></a>(像字符的正则表达式) 来指定哪些Unicode才可以被折叠。例如：瑞典单词 <code>å</code>,<code>ä</code>, <code>ö</code>, Å, <code>Ä</code>, 和 <code>Ö</code> 不能被折叠，你就可以设定为： <code>[^åäöÅÄÖ]</code> (<code>^</code> 表示 <em>不包含</em>)。这样就会对于所有的Unicode字符生效。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "swedish_folding": { <b class="conum">(1)</b>
          "type": "icu_folding",
          "unicodeSetFilter": "[^åäöÅÄÖ]"
        }
      },
      "analyzer": {
        "swedish_analyzer": { <b class="conum">(2)</b>
          "tokenizer": "icu_tokenizer",
          "filter":  [ "swedish_folding", "lowercase" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>`swedish_folding`语汇单元过滤器(token filters) 定制了 `icu_folding`语汇单元过滤器(token filters)来不处理那些大写和小写的瑞典单词。</p>
</li>
<li>
<p><code>swedish</code> 分析器首先分词，然后用`swedish_folding`语汇单元过滤器来折叠单词，最后把他们走转换为小写，除了被排除在外的单词： Å, <code>Ä</code>, 或者 <code>Ö</code>。</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="sorting-collations">排序和整理</h3>
<div class="paragraph">
<p>本章到目前为止，我们已经了解了怎么以搜索为目的去规范化词汇单元。  本章节中要考虑的最终用例是字符串排序。 </p>
</div>
<div class="paragraph">
<p>在 <a href="#multi-fields">[multi-fields]</a> （复数域）中，我们解释了 Elasticsearch  为什么不能在 <code>analyzed</code> （分析过）的字符串字段上排序，并演示了如何为同一个域创建 <em>复数域索引</em> ，其中 <code>analyzed</code> 域用来搜索， <code>not_analyzed</code> 域用来排序。 </p>
</div>
<div class="paragraph">
<p><code>analyzed</code> 域无法排序并不是因为使用了分析器，而是因为分析器将字符串拆分成了很多词汇单元，就像一个 <em>词汇袋</em> ，所以 Elasticsearch 不知道使用那一个词汇单元排序。</p>
</div>
<div class="paragraph">
<p>依赖于 <code>not_analyzed</code> 域来排序的话不是很灵活：这仅仅允许我们使用原始字符串这一确定的值排序。然而我们 <em>可以</em> 使用分析器来实现另外一种排序规则，只要你选择的分析器总是为每个字符串输出有且仅有一个的词汇单元。</p>
</div>
<div class="sect3">
<h4 id="case-insensitive-sorting">大小写敏感排序</h4>
<div class="paragraph">
<p>想象下我们有三个 <code>用户</code> 文档，文档的 <code>姓名</code> 域分别含有 <code>Boffey</code> 、 <code>BROWN</code> 和 <code>bailey</code> 。首先我们将使用在 <a href="#multi-fields">[multi-fields]</a> 中提到的技术，使用 <code>not_analyzed</code> 域来排序：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "mappings": {
    "user": {
      "properties": {
        "name": { <b class="conum">(1)</b>
          "type": "string",
          "fields": {
            "raw": { <b class="conum">(2)</b>
              "type":  "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>analyzed</code> <code>name</code> 域用来搜索。</p>
</li>
<li>
<p><code>not_analyzed</code> <code>name.raw</code> 域用来排序。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>我们可以索引一些文档用来测试排序：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/user/1
{ "name": "Boffey" }

PUT /my_index/user/2
{ "name": "BROWN" }

PUT /my_index/user/3
{ "name": "bailey" }

GET /my_index/user/_search?sort=name.raw</code></pre>
</div>
</div>
<div class="paragraph">
<p>运行这个搜索请求将会返回这样的文档排序： <code>BROWN</code> 、 <code>Boffey</code> 、 <code>bailey</code> 。 这个是 <em>词典排序</em>  跟 <em>字符串排序</em> 相反。基本上就是大写字母开头的字节要比小写字母开头的字节权重低，所以这些姓名是按照最低值优先排序。</p>
</div>
<div class="paragraph">
<p>这可能对计算机是合理的，但是对人来说并不是那么合理，人们更期望这些姓名按照字母顺序排序，忽略大小写。为了实现这个，我们需要把每个姓名按照我们想要的排序的顺序索引。</p>
</div>
<div class="paragraph">
<p>换句话来说，我们需要一个能输出单个小写词汇单元的分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "case_insensitive_sort": {
          "tokenizer": "keyword",    <b class="conum">(1)</b>
          "filter":  [ "lowercase" ] <b class="conum">(2)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>keyword</code> 分词器将输入的字符串原封不动的输出。 </p>
</li>
<li>
<p><code>lowercase</code> 分词过滤器将词汇单元转化为小写字母。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>使用  <code>大小写不敏感排序</code> 分析器替换后，现在我们可以将其用在我们的复数域：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/_mapping/user
{
  "properties": {
    "name": {
      "type": "string",
      "fields": {
        "lower_case_sort": { <b class="conum">(1)</b>
          "type":     "string",
          "analyzer": "case_insensitive_sort"
        }
      }
    }
  }
}

PUT /my_index/user/1
{ "name": "Boffey" }

PUT /my_index/user/2
{ "name": "BROWN" }

PUT /my_index/user/3
{ "name": "bailey" }

GET /my_index/user/_search?sort=name.lower_case_sort</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>name.lower_case_sort</code> 域将会为我们提供大小写不敏感排序。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>运行这个搜索请求会得到我们想要的文档排序： <code>bailey</code> 、 <code>Boffey</code> 、 <code>BROWN</code> 。</p>
</div>
<div class="paragraph">
<p>但是这个顺序是正确的么？它符合我门的期望所以看起来像是正确的，
但我们的期望可能受到这个事实的影响：这本书是英文的，我们的例子中使用的所有字母都属于到英语字母表。</p>
</div>
<div class="paragraph">
<p>如果我们添加一个德语姓名 <em>Böhm</em> 会怎样呢？</p>
</div>
<div class="paragraph">
<p>现在我们的姓名会返回这样的排序： <code>bailey</code> 、 <code>Boffey</code> 、 <code>BROWN</code> 、 <code>Böhm</code> 。 <code>Böhm</code> 会排在 <code>BROWN</code> 后面的原因是这些单词依然是按照它们表现的字节值排序的。 <code>r</code> 所存储的字节为 <code>0x72</code> ，而 <code>ö</code> 存储的字节值为 <code>0xF6</code> ，所以 <code>Böhm</code> 排在最后。每个字符的字节值都是历史的意外。</p>
</div>
<div class="paragraph">
<p>显然，默认排序顺序对于除简单英语之外的任何事物都是无意义的。事实上，没有完全“正确”的排序规则。这完全取决于你使用的语言。</p>
</div>
</div>
<div class="sect3">
<h4 id="_语言之间的区别">语言之间的区别</h4>
<div class="paragraph">
<p>每门语言都有自己的排序规则，并且  有时候甚至有多种排序规则。  这里有几个例子，我们前一小节中的四个名字在不同的上下文中是怎么排序的：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>英语：     <code>bailey</code> 、 <code>boffey</code> 、 <code>böhm</code> 、   <code>brown</code></p>
</li>
<li>
<p>德语：      <code>bailey</code> 、 <code>boffey</code> 、 <code>böhm</code> 、 <code>brown</code></p>
</li>
<li>
<p>德语电话簿：  <code>bailey</code> 、 <code>böhm</code> 、 <code>boffey</code> 、 <code>brown</code></p>
</li>
<li>
<p>瑞典语：          <code>bailey</code>, <code>boffey</code>, <code>brown</code>,  <code>böhm</code></p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>德语电话簿将 <code>böhm</code> 放在 <code>boffey</code> 的原因是 <code>ö</code> 和 <code>oe</code> 在处理名字和地点的时候会被看成同义词，所以 <code>böhm</code> 在排序时像是被写成了 <code>boehm</code> 。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="uca">Unicode 归类算法</h4>
<div class="paragraph">
<p>归类是将文本按预定义顺序排序的过程。  <em>Unicode 归类算法</em> 或称为 UCA （参见  <a href="http://www.unicode.org/reports/tr10/"><em>www.unicode.org/reports/tr10</em></a> ） 定义了一种将字符串按照在归类单元表中定义的顺序排序的方法（通常称为排序规则）。</p>
</div>
<div class="paragraph">
<p>UCA 还定义了 <em>默认 Unicode 排序规则元素表</em> 或称为 <em>DUCET</em> ， <em>DUCET</em> 为无论任何语言的所有 Unicode 字符定义了默认排序。如你所见，没有惟一一个正确的排序规则，所以 DUCET 让更少的人感到烦恼，且烦恼尽可能的小，但它还远不是解决所有排序烦恼的万能药。</p>
</div>
<div class="paragraph">
<p>而且，明显几乎每种语言都有自己的排序规则。大多时候使用
 DUCET 作为起点并且添加一些自定义规则用来处理每种语言的特性。</p>
</div>
<div class="paragraph">
<p>UCA 将字符串和排序规则作为输入，并输出二进制排序键。
将根据指定的排序规则对字符串集合进行排序转化为对其二进制排序键的简单比较。</p>
</div>
</div>
<div class="sect3">
<h4 id="_unicode_排序">Unicode 排序</h4>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>本节中描述的方法可能会在未来版本的 Elasticsearch 中更改。请查看 <a href="#icu-plugin"><code>icu</code> plugin</a> 文档的最新信息。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><code>icu_collation</code> 分词过滤器默认使用 DUCET 排序规则。这已经是对默认排序的改进了。想要使用 <code>icu_collation</code> 我们仅需要创建一个使用默认 <code>icu_collation</code> 过滤器的分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "ducet_sort": {
          "tokenizer": "keyword",
          "filter": [ "icu_collation" ] <b class="conum">(1)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>使用默认 DUCET 归类。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>通常，我们想要排序的字段就是我们想要搜索的字段，
因此我们使用与在 <a href="#case-insensitive-sorting">大小写敏感排序</a> 中使用的相同的复数域方法：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/_mapping/user
{
  "properties": {
    "name": {
      "type": "string",
      "fields": {
        "sort": {
          "type": "string",
          "analyzer": "ducet_sort"
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>使用这个映射， <code>name.sort</code> 域将会含有一个仅用来排序的键。我们没有指定某种语言，所以它会默认会使用 <a href="#uca">DUCET collation</a> 。</p>
</div>
<div class="paragraph">
<p>现在，我们可以重新索引我们的案例文档并测试排序：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/user/_bulk
{ "index": { "_id": 1 }}
{ "name": "Boffey" }
{ "index": { "_id": 2 }}
{ "name": "BROWN" }
{ "index": { "_id": 3 }}
{ "name": "bailey" }
{ "index": { "_id": 4 }}
{ "name": "Böhm" }

GET /my_index/user/_search?sort=name.sort</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>注意，每个文档返回的 <code>sort</code> 键，在前面的例子中看起来像 <code>brown</code> 和 <code>böhm</code> ，现在看起来像天书： <code>ᖔ乏昫တ倈⠀\u0001</code> 。原因是 <code>icu_collat​​ion</code> 过滤器输出键
仅用于有效分类，不用于任何其他目的。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>运行这个搜索请求反问的文档排序为： <code>bailey</code> 、 <code>Boffey</code> 、 <code>Böhm</code> 、 <code>BROWN</code> 。这个排序对英语和德语来说都正确，这已经是一种进步，但是它对德语电话簿和瑞典语来说还不正确。下一步我们为不同的语言自定义映射。</p>
</div>
</div>
<div class="sect3">
<h4 id="_指定语言">指定语言</h4>
<div class="paragraph">
<p>可以为特定的语言配置使用归类表的 <code>icu_collation</code> 过滤器，例如一个国家特定版本的语言，或者像德语电话簿之类的子集。
这个可以按照如下所示通过使用 <code>language</code> 、 <code>country</code> 、 和 <code>variant</code> 参数来创建自定义版本的分词过滤器：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">英语</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "language": "en" }</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">德语</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "language": "de" }</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">奥地利德语</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "language": "de", "country": "AT" }</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">德语电话簿</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "language": "de", "variant": "@collation=phonebook" }</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>你可以在一下网址阅读更多的 ICU 本地支持：
<a href="http://userguide.icu-project.org/locale" class="bare">http://userguide.icu-project.org/locale</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>这个例子演示怎么创建德语电话簿排序规则：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "number_of_shards": 1,
    "analysis": {
      "filter": {
        "german_phonebook": { <b class="conum">(1)</b>
          "type":     "icu_collation",
          "language": "de",
          "country":  "DE",
          "variant":  "@collation=phonebook"
        }
      },
      "analyzer": {
        "german_phonebook": { <b class="conum">(2)</b>
          "tokenizer": "keyword",
          "filter":  [ "german_phonebook" ]
        }
      }
    }
  },
  "mappings": {
    "user": {
      "properties": {
        "name": {
          "type": "string",
          "fields": {
            "sort": { <b class="conum">(3)</b>
              "type":     "string",
              "analyzer": "german_phonebook"
            }
          }
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>首先我们为德语电话薄创建一个自定义版本的  <code>icu_collation</code> 。</p>
</li>
<li>
<p>之后我们将其包装在自定义的分析器中。</p>
</li>
<li>
<p>并且为我们的 <code>name.sort</code> 域配置它。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>像我们之前那样重新索引并重新搜索：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/user/_bulk
{ "index": { "_id": 1 }}
{ "name": "Boffey" }
{ "index": { "_id": 2 }}
{ "name": "BROWN" }
{ "index": { "_id": 3 }}
{ "name": "bailey" }
{ "index": { "_id": 4 }}
{ "name": "Böhm" }

GET /my_index/user/_search?sort=name.sort</code></pre>
</div>
</div>
<div class="paragraph">
<p>现在返回的文档排序为： <code>bailey</code> 、 <code>Böhm</code> 、 <code>Boffey</code> 、 <code>BROWN</code> 。在德语电话簿归类中， <code>Böhm</code> 等同于 <code>Boehm</code> ，所以排在 <code>Boffey</code> 前面。</p>
</div>
<div class="sect4">
<h5 id="_多排序规则">多排序规则</h5>
<div class="paragraph">
<p>每种语言都可以使用复数域来支持对同一个域进行多规则排序：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index/_mapping/_user
{
  "properties": {
    "name": {
      "type": "string",
      "fields": {
        "default": {
          "type":     "string",
          "analyzer": "ducet" <b class="conum">(1)</b>
        },
        "french": {
          "type":     "string",
          "analyzer": "french" <b class="conum">(1)</b>
        },
        "german": {
          "type":     "string",
          "analyzer": "german_phonebook" <b class="conum">(1)</b>
        },
        "swedish": {
          "type":     "string",
          "analyzer": "swedish" <b class="conum">(1)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>我们需要为每个排序规则创建相应的分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>使用这个映射，只要按照 <code>name.french</code> 、  <code>name.german</code> 或 <code>name.swedish</code> 域排序，就可以为法语、德语和瑞典语用户正确的排序结果了。不支持的语言可以回退到使用 <code>name.default</code> 域，它使用 DUCET 排序顺序。</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_自定义排序">自定义排序</h4>
<div class="paragraph">
<p><code>icu_collation</code> 分词过滤器提供很多选项，不止 <code>language</code> 、 <code>country</code> 、和 <code>variant</code> ，这些选项可以用于定制排序算法。可用的选项有以下作用：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>忽略变音符号</p>
</li>
<li>
<p>顺序大写排先或排后，或忽略大小写</p>
</li>
<li>
<p>考虑或忽略标点符号和空白</p>
</li>
<li>
<p>将数字按字符串或数字值排序</p>
</li>
<li>
<p>自定义现有归类或定义自己的归类</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>这些选项的详细信息超出了本书的范围，更多的信息可以查询 <a href="https://github.com/elasticsearch/elasticsearch-analysis-icu">ICU plug-in documentation</a> 和 <a href="http://userguide.icu-project.org/collation/concepts">ICU project collation documentation</a> 。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="stemming">将单词还原为词根</h2>
<div class="sectionbody">
<div class="paragraph">
<p>大多数语言的单词都可以 <em>词形变化</em> ，意味着下列单词可以改变它们的形态用来表达不同的意思：</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>单复数变化</em> ：      fox 、foxes</p>
</li>
<li>
<p><em>时态变化</em> ：       pay 、 paid 、 paying</p>
</li>
<li>
<p><em>性别变化</em> ：      waiter 、 waitress</p>
</li>
<li>
<p><em>动词人称变化</em> ：      hear 、 hears</p>
</li>
<li>
<p><em>代词变化</em> ：        I 、 me 、 my</p>
</li>
<li>
<p><em>不规则变化</em> ：      ate 、 eaten</p>
</li>
<li>
<p><em>情景变化</em> ：        so be it 、 were it so</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>虽然词形变化有助于表达，但它干扰了检索，一个单一的词根 <em>词义</em> （或意义）可能被很多不同的字母序列表达。
英语是一种弱词形变化语言（你可以忽略词形变化并且能得到合理的搜索结果），但是一些其他语言是高度词形变化的并且需要额外的工作来保证高质量的搜索结果。</p>
</div>
<div class="paragraph">
<p><em>词干提取</em> 试图移除单词的变化形式之间的差别，从而达到将每个词都提取为它的词根形式。
例如 <code>foxes</code> 可能被提取为词根 <code>fox</code> ，移除单数和复数之间的区别跟我们移除大小写之间的区别的方式是一样的。</p>
</div>
<div class="paragraph">
<p>单词的词根形式甚至有可能不是一个真的单词，单词 <code>jumping</code> 和 <code>jumpiness</code> 或许都会被提取词干为 <code>jumpi</code> 。
这并没有什么问题&#8212;&#8203;只要在索引时和搜索时产生相同的词项，搜索会正常的工作。</p>
</div>
<div class="paragraph">
<p>如果词干提取很容易的话，那只要一个插件就够了。不幸的是，词干提取是一种遭受两种困扰的模糊的技术：词干弱提取和词干过度提取。</p>
</div>
<div class="paragraph">
<p><em>词干弱提取</em> 就是无法将同样意思的单词缩减为同一个词根。例如， <code>jumped</code> 和 <code>jumps</code> 可能被提取为 <code>jump</code> ，
但是 <code>jumping</code> 可能被提取为 <code>jumpi</code> 。弱词干提取会导致搜索时无法返回相关文档。</p>
</div>
<div class="paragraph">
<p><em>词干过度提取</em> 就是无法将不同含义的单词分开。例如， <code>general</code> 和 <code>generate</code>  可能都被提取为 <code>gener</code> 。
词干过度提取会降低精准度：不相干的文档会在不需要他们返回的时候返回。</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">词形还原</div>
<div class="paragraph">
<p>原词是一组相关词的规范形式，或词典形式&#8201;&#8212;&#8201;<code>paying</code> 、 <code>paid</code> 和 <code>pays</code> 的原词是 <code>pay</code> 。
通常原词很像与其相关的词，但有时也不像&#8201;&#8212;&#8201;<code>is</code> 、 <code>was</code> 、 <code>am</code> 和 <code>being</code> 的原词是 <code>be</code> 。</p>
</div>
<div class="paragraph">
<p>词形还原，很像词干提取，试图归类相关单词，但是它比词干提取先进一步的是它企图按单词的 <em>词义</em> ，或意义归类。
同样的单词可能表现出两种意思&#x2014;例如， <em>wake</em> 可以表现为 <em>to wake up</em> 或 <em>a funeral</em> 。然而词形还原试图区分两个词的词义，词干提取却会将其混为一谈。</p>
</div>
<div class="paragraph">
<p>词形还原是一种更复杂和高资源消耗的过程，它需要理解单词出现的上下文来决定词的意思。实践中，词干提取似乎比词形还原更高效，且代价更低。</p>
</div>
</div>
</div>
<div class="paragraph">
<p>首先我们会讨论下两个 Elasticsearch 使用的经典词干提取器 &#x2014; <a href="#algorithmic-stemmers">词干提取算法</a> 和 <a href="#dictionary-stemmers">字典词干提取器</a> &#x2014; 并且在 <a href="#choosing-a-stemmer">选择一个词干提取器</a> 讨论了怎么根据你的需要选择合适的词干提取器。
最后将在 <a href="#controlling-stemming">控制词干提取</a> 和 <a href="#stemming-in-situ">原形词干提取</a> 中讨论如何裁剪词干提取。</p>
</div>
<div class="sect2">
<h3 id="algorithmic-stemmers">词干提取算法</h3>
<div class="paragraph">
<p>Elasticsearch 中的大部分 stemmers （词干提取器）是基于算法的，它们提供了一系列规则用于将一个词提取为它的词根形式，例如剥离复数词末尾的 <code>s</code> 或 <code>es</code> 。提取单词词干时并不需要知道该词的任何信息。</p>
</div>
<div class="paragraph">
<p>这些基于算法的 stemmers 优点是：可以作为插件使用，速度快，占用内存少，有规律的单词处理效果好。缺点是：没规律的单词例如 <code>be</code> 、 <code>are</code> 、和 <code>am</code> ，或 <code>mice</code> 和 <code>mouse</code> 效果不好。</p>
</div>
<div class="paragraph">
<p>最早的一个基于算法的英文词干提取器是 Porter stemmer ，该英文词干提取器现在依然推荐使用。 Martin Porter 后来为了开发词干提取算法创建了 <a href="http://snowball.tartarus.org/">Snowball language</a> 网站， 很多 Elasticsearch 中使用的词干提取器就是用 Snowball 语言写的。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>{ref}/analysis-kstem-tokenfilter.html[<code>kstem</code> token filter] 是一款合并了词干提取算法和内置词典的英语分词过滤器。为了避免模糊词不正确提取，这个词典包含一系列根词单词和特例单词。 <code>kstem</code> 分词过滤器相较于 Porter 词干提取器而言不那么激进。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_使用基于算法的词干提取器">使用基于算法的词干提取器</h4>
<div class="paragraph">
<p>你可以使用 {ref}/analysis-porterstem-tokenfilter.html[<code>porter_stem</code>] 词干提取器或直接使用 {ref}/analysis-kstem-tokenfilter.html[<code>kstem</code>] 分词过滤器，或使用 {ref}/analysis-snowball-tokenfilter.html[<code>snowball</code>] 分词过滤器创建一个具体语言的 Snowball 词干提取器。所有基于算法的词干提取器都暴露了用来接受 <code>语言</code> 参数的统一接口： {ref}/analysis-stemmer-tokenfilter.html[<code>stemmer</code> token filter] 。</p>
</div>
<div class="paragraph">
<p>例如，假设你发现 <code>英语</code> 分析器使用的默认词干提取器太激进并且你想使它不那么激进。首先应在 {ref}/analysis-lang-analyzer.html[language analyzers] 查看 <code>英语</code> 分析器配置文件，配置文件展示如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_"
        },
        "english_keywords": {
          "type":       "keyword_marker", <b class="conum">(1)</b>
          "keywords":   []
        },
        "english_stemmer": {
          "type":       "stemmer",
          "language":   "english" <b class="conum">(2)</b>
        },
        "english_possessive_stemmer": {
          "type":       "stemmer",
          "language":   "possessive_english" <b class="conum">(2)</b>
        }
      },
      "analyzer": {
        "english": {
          "tokenizer":  "standard",
          "filter": [
            "english_possessive_stemmer",
            "lowercase",
            "english_stop",
            "english_keywords",
            "english_stemmer"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>keyword_marker</code> 分词过滤器列出那些不用被词干提取的单词。这个过滤器默认情况下是一个空的列表。</p>
</li>
<li>
<p><code>english</code> 分析器使用了两个词干提取器： <code>possessive_english</code> 词干提取器和 <code>english</code> 词干提取器。   所有格词干提取器会在任何词传递到  <code>english_stop</code> 、 <code>english_keywords</code> 和 <code>english_stemmer</code> 之前去除 <code>'s</code> 。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>重新审视下现在的配置，添加上以下修改，我们可以把这份配置当作新分析器的基本配置：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>修改 <code>english_stemmer</code> ，将  <code>english</code> （{ref}/analysis-porterstem-tokenfilter.html[<code>porter_stem</code>] 分词过滤器的映射）替换为 <code>light_english</code> （非激进的 {ref}/analysis-kstem-tokenfilter.html[<code>kstem</code>] 分词过滤器的映射）。</p>
</li>
<li>
<p>添加 <a href="#asciifolding-token-filter"><code>asciifolding</code></a> 分词过滤器用以移除外语的附加符号。</p>
</li>
<li>
<p>移除 <code>keyword_marker</code> 分词过滤器，因为我们不需要它。（我们会在 <a href="#controlling-stemming">控制词干提取</a> 中详细讨论它）</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>新定义的分析器会像下面这样:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-js" data-lang="js">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_"
        },
        "light_english_stemmer": {
          "type":       "stemmer",
          "language":   "light_english" <b class="conum">(1)</b>
        },
        "english_possessive_stemmer": {
          "type":       "stemmer",
          "language":   "possessive_english"
        }
      },
      "analyzer": {
        "english": {
          "tokenizer":  "standard",
          "filter": [
            "english_possessive_stemmer",
            "lowercase",
            "english_stop",
            "light_english_stemmer", <b class="conum">(1)</b>
            "asciifolding" <b class="conum">(2)</b>
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>将 <code>english</code> 词干提取器替换为非激进的 <code>light_english</code> 词干提取器</p>
</li>
<li>
<p>添加 <code>asciifolding</code> 分词过滤器</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="dictionary-stemmers">字典词干提取器</h3>
<div class="paragraph">
<p><em>字典词干提取器</em> 在工作机制上与 <a href="#algorithmic-stemmers">算法化词干提取器</a> 完全不同。  不同于应用一系列标准规则到每个词上，字典词干提取器只是简单地在字典里查找词。理论上可以给出比算法化词干提取器更好的结果。一个字典词干提取器应当可以：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>返回不规则形式如 <code>feet</code> 和 <code>mice</code> 的正确词干</p>
</li>
<li>
<p>区分出词形相似但词义不同的情形，比如 <code>organ</code> and <code>organization</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>实践中一个好的算法化词干提取器一般优于一个字典词干提取器。应该有以下两大原因：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">字典质量</dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>一个字典词干提取器再好也就跟它的字典一样。 据牛津英语字典网站估计，英语包含大约75万个单词（包含变音变形词）。电脑上的大部分英语字典只包含其中的 10% 。</p>
</div>
<div class="paragraph">
<p>词的含义随时光变迁。<code>mobility</code> 提取词干 <code>mobil</code> 先前可能讲得通，但现在合并进了手机可移动性的含义。字典需要保持最新，这是一项很耗时的任务。通常等到一个字典变得好用后，其中的部分内容已经过时。</p>
</div>
<div class="paragraph">
<p>字典词干提取器对于字典中不存在的词无能为力。而一个基于算法的词干提取器，则会继续应用之前的相同规则，结果可能正确或错误。</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1">大小与性能</dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>字典词干提取器需要加载所有词汇、 所有前缀，以及所有后缀到内存中。这会显著地消耗内存。找到一个词的正确词干，一般比算法化词干提取器的相同过程更加复杂。</p>
</div>
<div class="paragraph">
<p>依赖于不同的字典质量，去除前后缀的过程可能会更加高效或低效。低效的情形可能会明显地拖慢整个词干提取过程。</p>
</div>
<div class="paragraph">
<p>另一方面，算法化词干提取器通常更简单、轻量和快速。</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
如果你所使用的语言有比较好的算法化词干提取器，这通常是比一个基于字典的词干提取器更好的选择。对于算法化词干提取器效果比较差（或者压根没有）的语言，可以使用拼写检查（Hunspell）字典词干提取器，下一个章节会讨论。
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="hunspell">Hunspell 词干提取器</h3>
<div class="paragraph">
<p>Elasticsearch 提供了基于词典提取词干的

{ref}/analysis-hunspell-tokenfilter.html[<code>hunspell</code> 语汇单元过滤器（token filter）].
Hunspell <a href="http://hunspell.github.io/"><em>hunspell.github.io</em></a> 是一个 Open Office、LibreOffice、Chrome、Firefox、Thunderbird 等众多其它开源项目都在使用的拼写检查器。</p>
</div>
<div class="paragraph">
<p>可以从这里获取 Hunspell 词典 ：</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://extensions.openoffice.org/"><em>extensions.openoffice.org</em></a>: 下载解压 <code>.oxt</code> 后缀的文件。</p>
</li>
<li>
<p><a href="http://mzl.la/157UORf"><em>addons.mozilla.org</em></a>: 下载解压 <code>.xpi</code> 扩展文件。</p>
</li>
<li>
<p><a href="http://download.services.openoffice.org/contrib/dictionaries/">OpenOffice archive</a>: 下载解压 <code>.zip</code> 文件。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>一个 Hunspell 词典由两个文件组成&#8201;&#8212;&#8201;具有相同的文件名和两个不同的后缀&#8201;&#8212;&#8201;如
<code>en_US</code>&#x2014;和下面的两个后缀的其中一个：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>.dic</code></dt>
<dd>
<p>包含所有词根，采用字母顺序，再加上一个代表所有可能前缀和后缀的代码表 【集体称之为词缀( <em>affixes</em> 】</p>
</dd>
<dt class="hdlist1"><code>.aff</code></dt>
<dd>
<p>包含实际 <code>.dic</code> 文件每一行代码表对应的前缀和后缀转换</p>
</dd>
</dl>
</div>
<div class="sect3">
<h4 id="_安装一个词典">安装一个词典</h4>
<div class="paragraph">
<p>Hunspell 语汇单元过滤器在特定的 Hunspell 目录里寻找词典，
默认目录是 <code>./config/hunspell/</code> 。 <code>.dic</code> 文件和 <code>.aff</code> 文件应该要以子目录且按语言/区域的方式来命名。
例如，我们可以为美式英语创建一个 Hunspell 词干提取器，目录结构如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">config/
  └ hunspell/ <b class="conum">(1)</b>
      └ en_US/ <b class="conum">(2)</b>
          ├ en_US.dic
          ├ en_US.aff
          └ settings.yml <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>Hunspell 目录位置可以通过编辑 <code>config/elasticsearch.yml</code> 文件的：
<code>indices.analysis.hunspell.dictionary.location</code> 设置来修改。</p>
</li>
<li>
<p><code>en_US</code> 是这个区域的名字，也是我们传给 <code>hunspell</code> 语汇单元过滤器参数 <code>language</code> 值。</p>
</li>
<li>
<p>一个语言一个设置文件，下面的章节会具体介绍。</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_按语言设置">按语言设置</h4>
<div class="paragraph">
<p>在语言的目录设置文件 <code>settings.yml</code> 包含适用于所有字典内的语言目录的设置选项。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">---
ignore_case:          true
strict_affix_parsing: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>这些选项的意思如下：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>ignore_case</code></dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>Hunspell 目录默认是区分大小写的，如，姓氏 <code>Booker</code> 和名词 <code>booker</code> 是不同的词，所以应该分别进行词干提取。
也许让 <code>hunspell</code> 提取器区分大小写是一个好主意，不过也可能让事情变得复杂：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>一个句子的第一个词可能会被大写，因此感觉上会像是一个名词。</p>
</li>
<li>
<p>输入的文本可能全是大写，如果这样那几乎一个词都找不到。</p>
</li>
<li>
<p>用户也许会用小写来搜索名字，在这种情况下，大写开头的词将找不到。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>一般来说，设置参数 <code>ignore_case</code> 为 <code>true</code> 是一个好主意。</p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1"><code>strict_affix_parsing</code></dt>
<dd>
<p>词典的质量千差万别。 一些网上的词典的 <code>.aff</code> 文件有很多畸形的规则。
默认情况下，如果 Lucene 不能正常解析一个词缀(affix)规则， 它会抛出一个异常。
你可以通过设置 <code>strict_affix_parsing</code> 为 <code>false</code> 来告诉 Lucene 忽略错误的规则。</p>
</dd>
</dl>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">自定义词典</div>
<div class="paragraph">
<p>如果一个目录放置了多个词典 (<code>.dic</code> 文件)， 
他们会在加载时合并到一起。这可以让你以自定义的词典的方式对下载的词典进行定制：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">config/
  └ hunspell/
      └ en_US/  <b class="conum">(1)</b>
          ├ en_US.dic
          ├ en_US.aff <b class="conum">(2)</b>
          ├ custom.dic
          └ settings.yml</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>custom</code> 词典和 <code>en_US</code> 词典将合并到一起。</p>
</li>
<li>
<p>多个 <code>.aff</code> 文件是不允许的，因为会产生规则冲突。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><code>.dic</code> 文件和 <code>.aff</code> 文件的格式在这里讨论：
<a href="#hunspell-dictionary-format">Hunspell 词典格式</a> 。</p>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_创建一个_hunspell_语汇单元过滤器">创建一个 Hunspell 语汇单元过滤器</h4>
<div class="paragraph">
<p>一旦你在所有节点上安装好了词典，你就能像这样定义一个 <code>hunspell</code> 语汇单元过滤器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "en_US": {
          "type":     "hunspell",
          "language": "en_US" <b class="conum">(1)</b>
        }
      },
      "analyzer": {
        "en_US": {
          "tokenizer":  "standard",
          "filter":   [ "lowercase", "en_US" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>参数 <code>language</code> 和目录下对应的名称相同。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>你可以通过 <code>analyze</code> API 来测试这个新的分析器，
然后和 <code>english</code> 分析器比较一下它们的输出：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_analyze?analyzer=en_US <b class="conum">(1)</b>
reorganizes

GET /_analyze?analyzer=english <b class="conum">(2)</b>
reorganizes</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>返回 <code>organize</code></p>
</li>
<li>
<p>返回 <code>reorgan</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>在前面的例子中，<code>hunspell</code> 提取器有一个有意思的事情，它不仅能移除前缀还能移除后缀。大多数算法词干提取仅能移除后缀。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>Hunspell 词典会占用几兆的内存。幸运的是，Elasticsearch 每个节点只会创建一个词典的单例。
所有的分片都会使用这个相同的 Hunspell 分析器。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="hunspell-dictionary-format">Hunspell 词典格式</h4>
<div class="paragraph">
<p>尽管使用 <code>hunspell</code> 不必了解 Hunspell 词典的格式， 
不过了解格式可以帮助我们编写自己的自定义的词典。其实很简单。</p>
</div>
<div class="paragraph">
<p>例如，在美式英语词典（US English dictionary），<code>en_US.dic</code> 文件包含了一个包含词 <code>analyze</code> 的实体，看起来如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">analyze/ADSG</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>en_US.aff</code> 文件包含了一个针对标记 <code>A</code> 、 <code>G</code> 、<code>D</code> 和 <code>S</code> 的前后缀的规则。
其中应该只有一个能匹配，每一个规则的格式如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[type] [flag] [letters to remove] [letters to add] [condition]</code></pre>
</div>
</div>
<div class="paragraph">
<p>例如，下面的后缀 (<code>SFX</code>) 规则 <code>D</code> 。它是说，当一个词由一个辅音 (除了 <code>a</code> 、<code>e</code> 、<code>i</code> 、<code>o</code> 或 <code>u</code> 外的任意音节)
 后接一个 <code>y</code> ，那么它可以移除 <code>y</code> 和添加 <code>ied</code> 结尾 （如，<code>ready</code> &#8594; <code>readied</code> ）。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">SFX    D      y   ied  [^aeiou]y</code></pre>
</div>
</div>
<div class="paragraph">
<p>前面提到的 <code>A</code> 、 <code>G</code> 、<code>D</code> 和 <code>S</code> 标记对应规则如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">SFX D Y 4
SFX D   0     d          e <b class="conum">(1)</b>
SFX D   y     ied        [^aeiou]y
SFX D   0     ed         [^ey]
SFX D   0     ed         [aeiou]y

SFX S Y 4
SFX S   y     ies        [^aeiou]y
SFX S   0     s          [aeiou]y
SFX S   0     es         [sxzh]
SFX S   0     s          [^sxzhy] <b class="conum">(2)</b>

SFX G Y 2
SFX G   e     ing        e <b class="conum">(3)</b>
SFX G   0     ing        [^e]

PFX A Y 1
PFX A   0     re         . <b class="conum">(4)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>analyze</code> 以一个 <code>e</code> 结尾，所以它可以添加一个 <code>d</code> 变成 <code>analyzed</code> 。</p>
</li>
<li>
<p><code>analyze</code> 不是由 <code>s</code> 、<code>x</code> 、<code>z</code> 、<code>h</code> 或 <code>y</code> 结尾，所以，它可以添加一个 <code>s</code> 变成 <code>analyzes</code> 。</p>
</li>
<li>
<p><code>analyze</code> 以一个 <code>e</code> 结尾，所以，它可以移除 <code>e</code> 和添加 <code>ing</code> 然后变成 <code>analyzing</code> 。</p>
</li>
<li>
<p>可以添加前缀 <code>re</code> 来形成 <code>reanalyze</code> 。这个规则可以组合后缀规则一起形成： <code>reanalyzes</code> 、<code>reanalyzed</code> 、
<code>reanalyzing</code> 。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>了解更多关于 Hunspell 的语法，可以前往 <a href="http://sourceforge.net/projects/hunspell/files/Hunspell/Documentation/">Hunspell 文档</a> 。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="choosing-a-stemmer">选择一个词干提取器</h3>
<div class="paragraph">
<p>在文档
{ref}/analysis-stemmer-tokenfilter.html[<code>stemmer</code>] token filter
里面列出了一些针对语言的若干词干提取器。
就英语来说我们有如下提取器：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>english</code></dt>
<dd>
<p>{ref}/analysis-porterstem-tokenfilter.html[<code>porter_stem</code>] 语汇单元过滤器（token filter）。</p>
</dd>
<dt class="hdlist1"><code>light_english</code></dt>
<dd>
<p>{ref}/analysis-kstem-tokenfilter.html[<code>kstem</code>] 语汇单元过滤器（token filter）。</p>
</dd>
<dt class="hdlist1"><code>minimal_english</code></dt>
<dd>
<p>Lucene 里面的 <code>EnglishMinimalStemmer</code> ，用来移除复数。</p>
</dd>
<dt class="hdlist1"><code>lovins</code></dt>
<dd>
<p>基于 {ref}/analysis-snowball-tokenfilter.html[Snowball] 的
<a href="http://snowball.tartarus.org/algorithms/lovins/stemmer.html">Lovins</a>
提取器, 第一个词干提取器。</p>
</dd>
<dt class="hdlist1"><code>porter</code></dt>
<dd>
<p>基于 {ref}/analysis-snowball-tokenfilter.html[Snowball] 的
<a href="http://snowball.tartarus.org/algorithms/porter/stemmer.html">Porter</a> 提取器。</p>
</dd>
<dt class="hdlist1"><code>porter2</code></dt>
<dd>
<p>基于 {ref}/analysis-snowball-tokenfilter.html[Snowball] 的
<a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">Porter2</a> 提取器。</p>
</dd>
<dt class="hdlist1"><code>possessive_english</code></dt>
<dd>
<p>Lucene 里面的 <code>EnglishPossessiveFilter</code> ，移除 <code>'s</code></p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Hunspell 词干提取器也要纳入到上面的列表中，还有多种英文的词典可用。</p>
</div>
<div class="paragraph">
<p>有一点是可以肯定的：当一个问题存在多个解决方案的时候，这意味着没有一个解决方案充分解决这个问题。
这一点同样体现在词干提取上&#8201;&#8212;&#8201;每个提取器使用不同的方法不同程度的对单词进行了弱提取或是过度提取。</p>
</div>
<div class="paragraph">
<p>在 <code>stemmer</code> 文档 中，使用粗体高亮了每一个语言的推荐的词干提取器，
通常是因为它提供了一个在性能和质量之间合理的妥协。也就是说，推荐的词干提取器也许不适用所有场景。
关于哪个是最好的词干提取器，不存在一个唯一的正确答案&#8201;&#8212;&#8201;它要看你具体的需求。
这里有3个方面的因素需要考虑在内：
性能、质量、程度。</p>
</div>
<div class="sect3">
<h4 id="stemmer-performance">提取性能</h4>
<div class="paragraph">
<p>算法提取器一般来说比 Hunspell 提取器快4到5倍。
<code>`Handcrafted'' 算法提取器通常（不是永远） 要比 Snowball 快或是差不多。
比如，`porter_stem</code> 语汇单元过滤器（token filter）就明显要比基于 Snowball 实现的 Porter 提取器要快的多。</p>
</div>
<div class="paragraph">
<p>Hunspell 提取器需要加载所有的词典、前缀和后缀表到内存，可能需要消耗几兆的内存。而算法提取器，由一点点代码组成，只需要使用很少内存。</p>
</div>
</div>
<div class="sect3">
<h4 id="stemmer-quality">提取质量</h4>
<div class="paragraph">
<p>所有的语言，除了世界语（Esperanto）都是不规范的。
最日常用语使用的词往往不规则，而更正式的书面用语则往往遵循规律。
一些提取算法经过多年的开发和研究已经能够产生合理的高质量的结果了，其他人只需快速组装做很少的研究就能解决大部分的问题了。</p>
</div>
<div class="paragraph">
<p>虽然 Hunspell 提供了精确地处理不规则词语的承诺，但在实践中往往不足。
一个基于词典的提取器往往取决于词典的好坏。如果 Hunspell 碰到的这个词不在词典里，那它什么也不能做。
Hunspell 需要一个广泛的、高质量的、最新的词典以产生好的结果；这样级别的词典可谓少之又少。
另一方面，一个算法提取器，将愉快的处理新词而不用为新词重新设计算法。</p>
</div>
<div class="paragraph">
<p>如果一个好的算法词干提取器可用于你的语言，那明智的使用它而不是 Hunspell。它会更快并且消耗更少内存，并且会产生和通常一样好或者比 Hunspell 等价的结果.</p>
</div>
<div class="paragraph">
<p>如果精度和可定制性对你很重要，那么你需要（和有精力）来维护一个自定义的词典，那么 Hunspell 会给你比算法提取器更大的灵活性。 (查看
<a href="#controlling-stemming">控制词干提取</a> 来了解可用于任何词干提取器的自定义技术。)</p>
</div>
</div>
<div class="sect3">
<h4 id="stemmer-degree">提取程度</h4>
<div class="paragraph">
<p>不同的词干提取器会将词弱提取或过度提取到一定的程度。  <code>light_</code>
提取器提干力度不及标准的提取器。 <code>minimal_</code> 提取器同样也不那么积极。Hunspell 提取力度要激进一些。</p>
</div>
<div class="paragraph">
<p>是否想要积极提取还是轻量提取取决于你的场景。如果你的搜索结果是要用于聚类算法，你可能会希望匹配的更广泛一点（因此，提取力度要更大一点）。
如果你的搜索结果是面向最终用户，轻量的提取一般会产生更好的结果。对搜索来说，将名称和形容词提干比动词提干更重要，当然这也取决于语言。</p>
</div>
<div class="paragraph">
<p>另外一个要考虑的因素就是你的文档集的大小。
一个只有 10,000 个产品的小集合，你可能要更激进的提干来确保至少匹配到一些文档。
如果你的文档集很大，使用轻量的弱提取可能会得到更好的匹配结果。</p>
</div>
</div>
<div class="sect3">
<h4 id="_做一个选择">做一个选择</h4>
<div class="paragraph">
<p>从推荐的一个词干提取器出发，如果它工作的很好，那没有什么需要调整的。如果不是，你将需要花点时间来调查和比较该语言可用的各种不同提取器，
来找到最适合你目的的那一个。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="controlling-stemming">控制词干提取</h3>
<div class="paragraph">
<p>开箱即用的词干提取方案永远也不可能完美。
尤其是算法提取器，他们可以愉快的将规则应用于任何他们遇到的词，包含那些你希望保持独立的词。
也许，在你的场景，保持独立的 <code>skies</code> 和 <code>skiing</code> 是重要的，你不希望把他们提取为 <code>ski</code> （正如 <code>english</code> 分析器那样）。</p>
</div>
<div class="paragraph">
<p>语汇单元过滤器 {ref}/analysis-keyword-marker-tokenfilter.html[<code>keyword_marker</code>] 和
{ref}/analysis-stemmer-override-tokenfilter.html[<code>stemmer_override</code>] 
能让我们自定义词干提取过程。</p>
</div>
<div class="sect3">
<h4 id="preventing-stemming">阻止词干提取</h4>
<div class="paragraph">
<p>语言分析器（查看 <a href="#configuring-language-analyzers">配置语言分析器</a>）的参数 <a href="#stem-exclusion"><code>stem_exclusion</code></a>
允许我们指定一个词语列表，让他们不被词干提取。</p>
</div>
<div class="paragraph">
<p>在内部，这些语言分析器使用
{ref}/analysis-keyword-marker-tokenfilter.html[<code>keyword_marker</code> 语汇单元过滤器]
来标记这些词语列表为 <em>keywords</em> ，用来阻止后续的词干提取过滤器来触碰这些词语。</p>
</div>
<div class="paragraph">
<p>例如，我们创建一个简单自定义分析器，使用
{ref}/analysis-porterstem-tokenfilter.html[<code>porter_stem</code>] 语汇单元过滤器，同时阻止 <code>skies</code> 的词干提取：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "no_stem": {
          "type": "keyword_marker",
          "keywords": [ "skies" ] <b class="conum">(1)</b>
        }
      },
      "analyzer": {
        "my_english": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "no_stem",
            "porter_stem"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>参数 <code>keywords</code> 可以允许接收多个词语。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>使用 <code>analyze</code> API 来测试，可以看到词 <code>skies</code> 没有被提取：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_analyze?analyzer=my_english
sky skies skiing skis <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>返回: <code>sky</code>, <code>skies</code>, <code>ski</code>, <code>ski</code></p>
</li>
</ol>
</div>
<div id="keyword-path" class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>虽然语言分析器只允许我们通过参数 <code>stem_exclusion</code> 指定一个词语列表来排除词干提取，
不过 <code>keyword_marker</code> 语汇单元过滤器同样还接收一个 <code>keywords_path</code> 参数允许我们将所有的关键字存在一个文件。
这个文件应该是每行一个字，并且存在于集群的每个节点。查看 <a href="#updating-stopwords">更新停用词（Updating Stopwords）</a> 了解更新这些文件的提示。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="customizing-stemming">自定义提取</h4>
<div class="paragraph">
<p>在上面的例子中，我们阻止了 <code>skies</code> 被词干提取，但是也许我们希望他能被提干为 <code>sky</code> 。  The
{ref}/analysis-stemmer-override-tokenfilter.html[<code>stemmer_override</code>] 语汇单元过滤器允许我们指定自定义的提取规则。
与此同时，我们可以处理一些不规则的形式，如：<code>mice</code> 提取为 <code>mouse</code> 和 <code>feet</code> 到 <code>foot</code> ：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "custom_stem": {
          "type": "stemmer_override",
          "rules": [ <b class="conum">(1)</b>
            "skies=&gt;sky",
            "mice=&gt;mouse",
            "feet=&gt;foot"
          ]
        }
      },
      "analyzer": {
        "my_english": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "custom_stem", <b class="conum">(2)</b>
            "porter_stem"
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_english
The mice came down from the skies and ran over my feet <b class="conum">(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>规则来自 <code>original&#8658;stem</code> 。</p>
</li>
<li>
<p><code>stemmer_override</code> 过滤器必须放置在词干提取器之前。</p>
</li>
<li>
<p>返回 <code>the</code>, <code>mouse</code>, <code>came</code>, <code>down</code>, <code>from</code>, <code>the</code>, <code>sky</code>,
<code>and</code>, <code>ran</code>, <code>over</code>, <code>my</code>, <code>foot</code> 。</p>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
正如 <code>keyword_marker</code> 语汇单元过滤器，规则可以被存放在一个文件中，通过参数 <code>rules_path</code> 来指定位置。
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="stemming-in-situ">原形词干提取</h3>
<div class="paragraph">
<p>为了完整地 完成本章的内容，我们将讲解如何将已提取词干的词和原词索引到同一个字段中。举个例子，分析句子 <em>The quick foxes jumped</em> 将会得到以下词项：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: (the)
Pos 2: (quick)
Pos 3: (foxes,fox) <b class="conum">(1)</b>
Pos 4: (jumped,jump) <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>已提取词干的形式和未提取词干的形式位于相同的位置。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Warning：使用此方法前请先阅读 <a href="#stemming-in-situ-good-idea">原形词干提取是个好主意吗</a> 。</p>
</div>
<div class="paragraph">
<p>为了归档词干提取出的 <em>原形</em> ，我们将使用 <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-repeat-tokenfilter.html"><code>keyword_repeat</code></a> 过滤器，跟 <code>keyword_marker</code> 过滤器 ( see <a href="#preventing-stemming">阻止词干提取</a> ) 一样，它把每一个词项都标记为关键词，以防止后续词干提取器对其修改。但是，它依然会在相同位置上重复词项，并且这个重复的词项 <strong>是</strong> 提取的词干。</p>
</div>
<div class="paragraph">
<p>单独使用 <code>keyword_repeat</code> token 过滤器将得到以下结果：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: (the,the) <b class="conum">(1)</b>
Pos 2: (quick,quick) <b class="conum">(1)</b>
Pos 3: (foxes,fox)
Pos 4: (jumped,jump)</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>提取词干前后的形式一样，所以只是不必要的重复。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>为了防止提取和未提取词干形式相同的词项中的无意义重复，我们增加了组合的 {ref}/analysis-unique-tokenfilter.html[<code>unique</code>] 语汇单元过滤器  ：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "unique_stem": {
          "type": "unique",
          "only_on_same_position": true <b class="conum">(1)</b>
        }
      },
      "analyzer": {
        "in_situ": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "keyword_repeat", <b class="conum">(2)</b>
            "porter_stem",
            "unique_stem" <b class="conum">(3)</b>
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>设置 <code>unique</code> 类型语汇单元过滤器，是为了只有当重复语汇单元出现在相同位置时，移除它们。</p>
</li>
<li>
<p>语汇单元过滤器必须出现在词干提取器之前。</p>
</li>
<li>
<p><code>unique_stem</code> 过滤器是在词干提取器完成之后移除重复词项。</p>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="stemming-in-situ-good-idea">原形词干提取是个好主意吗</h4>
<div class="paragraph">
<p>用户喜欢 <em>原形</em> 词干提取这个主意：``如果我可以只用一个组合字段，为什么还要分别存一个未提取词干和已提取词干的字段呢？'' 但这是一个好主意吗？答案一直都是否定的。因为有两个问题：</p>
</div>
<div class="paragraph">
<p>第一个问题是无法区分精准匹配和非精准匹配。本章中，我们看到了多义词经常会被展开成相同的词干词：<code>organs</code> 和 <code>organization</code> 都会被提取为 <code>organ</code> 。</p>
</div>
<div class="paragraph">
<p>在 <a href="#using-language-analyzers">使用语言分析器</a> 我们展示了如何整合一个已提取词干属性的查询(为了增加召回率)和一个未提取词干属性的查询（为了提升相关度）。 当提取和未提取词干的属性相互独立时，单个属性的贡献可以通过给其中一个属性增加boost值来优化(参见 <a href="#prioritising-clauses">[prioritising-clauses]</a> )。相反地，如果已提取和未提取词干的形式置于同一个属性，就没有办法来优化搜索结果了。</p>
</div>
<div class="paragraph">
<p>第二个问题是，必须搞清楚  相关度分值是否如何计算的。在 <a href="#relevance-intro">[relevance-intro]</a> 我们解释了部分计算依赖于逆文档频率（IDF）—— 即一个词在索引库的所有文档中出现的频繁程度。 在一个包含文本 <code>jump jumped jumps</code> 的文档上使用原形词干提取，将得到下列词项：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: (jump)
Pos 2: (jumped,jump)
Pos 3: (jumps,jump)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>jumped</code> 和 <code>jumps</code> 各出现一次，所以有正确的IDF值；<code>jump</code> 出现了3次，作为一个搜索词项，与其他未提取词干的形式相比，这明显降低了它的IDF值。</p>
</div>
<div class="paragraph">
<p>基于这些原因，我们不推荐使用原形词干提取。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="stopwords">停用词: 性能与精度</h2>
<div class="sectionbody">
<div class="paragraph">
<p>从早期的信息检索到如今， 我们已习惯于磁盘空间和内存被限制为很小一部分，所以
必须使你的索引尽可能小。 每个字节都意味着巨大的性能提升。  (查看 <a href="#stemming">将单词还原为词根</a> ) 词干提取的重要性不仅是因为它让搜索的内容更广泛、让检索的能力更深入，还因为它是压缩索引空间的工具。</p>
</div>
<div class="paragraph">
<p>一种最简单的减少索引大小的方法就是 <em>索引更少的词</em>。 有些词要比其他词更重要，只索引那些更重要的词来可以大大减少索引的空间。</p>
</div>
<div class="paragraph">
<p>那么哪些词条可以被过滤呢？ 我们可以简单分为两组:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">低频词（Low-frequency terms）</dt>
<dd>
<p>在文档集合中相对出现较少的词，因为它们稀少，所以它们的权重值更高。</p>
</dd>
<dt class="hdlist1">高频词（High-frequency terms）</dt>
<dd>
<p>在索引下的文档集合中出现较多的常用词，例如 <code>the</code>、<code>and</code>、和`is`。 这些词的权重小，对相关度评分影响不大。</p>
</dd>
</dl>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>当然，频率实际上是个可以衡量的标尺而不是非 <em>高</em> 即 <em>低</em> 的标签。我们可以在标尺的任何位置选取一个标准，低于这个标准的属于低频词，高于它的属于高频词。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>词项到底是低频或是高频取决于它们所处的文档。单词 <code>and</code> 如果在所有都是中文的文档里可能是个低频词。在关于数据库的文档集合里，单词 <code>database</code> 可能是一个高频词项，它对搜索这个特定集合毫无帮助。</p>
</div>
<div class="paragraph">
<p>每种语言都存在一些非常常见的单词，它们对搜索没有太大价值。在 Elasticsearch 中，英语默认的停用词为:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>a, an, and, are, as, at, be, but, by, for, if, in, into, is, it,
no, not, of, on, or, such, that, the, their, then, there, these,
they, this, to, was, will, with</pre>
</div>
</div>
<div class="paragraph">
<p>这些 <em>停用词</em> 通常在索引前就可以被过滤掉，同时对检索的负面影响不大。但是这样做真的是一个较好的解决方案？</p>
</div>
<div class="sect2">
<h3 id="pros-cons-stopwords">停用词的优缺点</h3>
<div class="paragraph">
<p>现在我们拥有更大的磁盘空间，更多内存，并且还有更好的压缩算法。
将之前的 33 个常见词从索引中移除，每百万文档只能节省 4MB 空间。 所以使用停用词减少索引大小不再是一个有效的理由。
(不过这种说法还有一点需要注意，我们在 <a href="#stopwords-phrases">停用词与短语查询</a> 讨论。)</p>
</div>
<div class="paragraph">
<p>在此基础上，从索引里将这些词移除会使我们降低某种类型的搜索能力。将前面这些所列单词移除会让我们难以完成以下事情：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>区分 <em>happy</em> 和 <em>not happy</em>。</p>
</li>
<li>
<p>搜索乐队名称 The The。</p>
</li>
<li>
<p>查找莎士比亚的名句 ``To be, or not to be'' （生存还是毁灭)。</p>
</li>
<li>
<p>使用挪威的国家代码: <code>no</code>。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>移除停用词的最主要好处是性能，假设我们在个具有上百万文档的索引中搜索单词 <code>fox</code>。或许 <code>fox 只在其中 20 个文档中出现，也就是说 Elasticsearch 需要计算 20 个文档的相关度评分 `_score `从而排出前十。现在我们把搜索条件改为 `the OR fox</code>，几乎所有的文件都包含 <code>the</code> 这个词，也就是说 Elasticsearch 需要为所有一百万文档计算评分 <code>_score</code>。 由此可见第二个查询肯定没有第一个的结果好。</p>
</div>
<div class="paragraph">
<p>幸运的是，我们可以用来保持常用词搜索，同时还可以保持良好的性能。首先我们一块学习如何使用停用词。</p>
</div>
</div>
<div class="sect2">
<h3 id="using-stopwords">使用停用词</h3>
<div class="paragraph">
<p>移除停用词的工作是由 <code>stop</code> 停用词过滤器完成的，可以通过创建自定义的分析器来使用它（参见 使用停用词过滤器{ref}/analysis-stop-tokenfilter.html[<code>stop</code> 停用词过滤器])。但是，也有一些自带的分析器预置使用停用词过滤器：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">{ref}/analysis-lang-analyzer.html[语言分析器]</dt>
<dd>
<p>每个语言分析器默认使用与该语言相适的停用词列表，例如：<code>english</code> 英语分析器使用 <code><em>english</em></code> 停用词列表。</p>
</dd>
<dt class="hdlist1">{ref}/analysis-standard-analyzer.html[<code>standard</code> 标准分析器]</dt>
<dd>
<p>默认使用空的停用词列表：<code><em>none</em></code> ，实际上是禁用了停用词。</p>
</dd>
<dt class="hdlist1">{ref}/analysis-pattern-analyzer.html[<code>pattern</code> 模式分析器]</dt>
<dd>
<p>默认使用空的停用词列表：为 <code><em>none</em></code> ，与 <code>standard</code> 分析器类似。</p>
</dd>
</dl>
</div>
<div class="sect3">
<h4 id="_停用词和标准分析器_stopwords_and_the_standard_analyzer">停用词和标准分析器（Stopwords and the Standard Analyzer）</h4>
<div class="paragraph">
<p>为了让标准分析器能与自定义停用词表连用，我们要做的只需创建一个分析器的配置好的版本，然后将停用词列表传入：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": { <b class="conum">(1)</b>
          "type": "standard", <b class="conum">(2)</b>
          "stopwords": [ "and", "the" ] <b class="conum">(3)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>自定义的分析器名称为 <code>my_analyzer</code> 。</p>
</li>
<li>
<p>这个分析器是一个标准 <code>standard</code> 分析器，进行了一些自定义配置。</p>
</li>
<li>
<p>过滤掉的停用词包括 <code>and</code> 和 <code>the</code> 。</p>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
任何语言分析器都可以使用相同的方式配置自定义停用词。
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="maintaining-positions">保持位置（Maintaining Positions）</h4>
<div class="paragraph">
<p><code>analyzer</code> API的输出结果很有趣:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_analyze?analyzer=my_analyzer
The quick and the dead</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
   "tokens": [
      {
         "token":        "quick",
         "start_offset": 4,
         "end_offset":   9,
         "type":         "&lt;ALPHANUM&gt;",
         "position":     1 <b class="conum">(1)</b>
      },
      {
         "token":        "dead",
         "start_offset": 18,
         "end_offset":   22,
         "type":         "&lt;ALPHANUM&gt;",
         "position":     4
      }
   ]
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>position</code> 标记每个词汇单元的位置。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>停用词如我们期望被过滤掉了，但有趣的是两个词项的位置 <code>position</code> 没有变化：<code>quick</code> 是原句子的第二个词，<code>dead</code> 是第五个。这对短语查询十分重要，因为如果每个词项的位置被调整了，一个短语查询 <code>quick dead</code> 会与以上示例中的文档错误匹配。</p>
</div>
</div>
<div class="sect3">
<h4 id="specifying-stopwords">指定停用词（Specifying Stopwords）</h4>
<div class="paragraph">
<p>停用词可以以内联的方式传入，就像我们在前面的例子中那样，通过指定数组:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">"stopwords": [ "and", "the" ]</code></pre>
</div>
</div>
<div class="paragraph">
<p>特定语言的默认停用词，可以通过使用 <code><em>lang</em></code> 符号来指定:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">"stopwords": "_english_"</code></pre>
</div>
</div>
<div class="paragraph">
<p>TIP:
Elasticsearch 中预定义的与语言相关的停用词列表可以在文档"languages", "predefined stopword lists for"){ref}/analysis-stop-tokenfilter.html[<code>stop</code> 停用词过滤器] 中找到。</p>
</div>
<div class="paragraph">
<p>停用词可以通过指定一个特殊列表 <code><em>none</em></code> 来禁用。例如，使用 <code><em>english</em></code> 分析器而不使用停用词，可以通过以下方式做到：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english": {
          "type":      "english", <b class="conum">(1)</b>
          "stopwords": "_none_" <b class="conum">(2)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>my_english</code> 分析器是基于 <code>english</code> 分析器。</p>
</li>
<li>
<p>但禁用了停用词。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>最后，停用词还可以使用一行一个单词的格式保存在文件中。此文件必须在集群的所有节点上，并且通过 <code>stopwords_path</code>  参数设置路径:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english": {
          "type":           "english",
          "stopwords_path": "stopwords/english.txt" <b class="conum">(1)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>停用词文件的路径，该路径相对于 Elasticsearch 的 <code>config</code> 目录。</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="stop-token-filter">使用停用词过滤器（Using the stop Token Filter）</h4>
<div class="paragraph">
<p>当你创建 <code>custom</code> 分析器时候，可以组合多个 {ref}/analysis-stop-tokenfilter.html[<code>stop</code> 停用词过滤器] 分词器。例如：我们想要创建一个西班牙语的分析器:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>自定义停用词列表</p>
</li>
<li>
<p><code>light_spanish</code> 词干提取器</p>
</li>
<li>
<p>在 <code>asciifolding</code> 词汇单元过滤器中除去附加符号</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>我们可以通过以下设置完成:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "spanish_stop": {
          "type":        "stop",
          "stopwords": [ "si", "esta", "el", "la" ]  <b class="conum">(1)</b>
        },
        "light_spanish": { <b class="conum">(2)</b>
          "type":     "stemmer",
          "language": "light_spanish"
        }
      },
      "analyzer": {
        "my_spanish": {
          "tokenizer": "spanish",
          "filter": [ <b class="conum">(3)</b>
            "lowercase",
            "asciifolding",
            "spanish_stop",
            "light_spanish"
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>停用词过滤器采用与 <code>standard</code> 分析器相同的参数 <code>stopwords</code> 和 <code>stopwords_path</code> 。</p>
</li>
<li>
<p>参见 算法提取器（Algorithmic Stemmers）。</p>
</li>
<li>
<p>过滤器的顺序非常重要，下面会进行解释。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>我们将 <code>spanish_stop</code> 过滤器放置在 <code>asciifolding</code> 过滤器之后.这意味着以下三个词组 <code>esta</code> 、<code>ésta</code> 、<code>está</code> ，先通过 <code>asciifolding</code> 过滤器过滤掉特殊字符变成了 <code>esta</code> ，随后使用停用词过滤器会将 <code>esta</code> 去除。
如果我们只想移除 <code>esta</code> 和 <code>ésta</code> ，但是 <code>está</code> 不想移除。必须将 <code>spanish_stop</code> 过滤器放置在 <code>asciifolding</code> 之前，并且需要在停用词中指定 <code>esta</code> 和 <code>ésta</code> 。</p>
</div>
</div>
<div class="sect3">
<h4 id="updating-stopwords">更新停用词（Updating Stopwords）</h4>
<div class="paragraph">
<p>想要更新分析器的停用词列表有多种方式， 分析器在创建索引时，当集群节点重启时候，或者关闭的索引重新打开的时候。</p>
</div>
<div class="paragraph">
<p>如果你使用 <code>stopwords</code> 参数以内联方式指定停用词，那么你只能通过关闭索引，更新分析器的配置{ref}/indices-update-settings.html#update-settings-analysis[update index settings API]，然后在重新打开索引才能更新停用词。</p>
</div>
<div class="paragraph">
<p>如果你使用 <code>stopwords_path</code> 参数指定停用词的文件路径 ，那么更新停用词就简单了。你只需更新文件(在每一个集群节点上)，然后通过两者之中的任何一个操作来强制重新创建分析器:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>关闭和重新打开索引
(参考 {ref}/indices-open-close.html[索引的开与关])，</p>
</li>
<li>
<p>一一重启集群下的每个节点。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>当然，更新的停用词不会改变任何已经存在的索引。这些停用词的只适用于新的搜索或更新文档。如果要改变现有的文档，则需要重新索引数据。参加 <a href="#reindex">[reindex]</a> 。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="stopwords-performance">停用词与性能</h3>
<div class="paragraph">
<p>保留停用词最大的缺点就影响搜索性能。使用 Elasticsearch 进行全文搜索，它需要为所有匹配的文档计算相关度评分 <code>_score</code> 从而返回最相关的前 10 个文档。</p>
</div>
<div class="paragraph">
<p>通常大多数的单词在所有文档中出现的频率低于0.1％，但是有少数词（例如 <code>the</code> ）几乎存在于所有的文档中。假设有一个索引含有100万个文档，查询 <code>quick brown fox</code> 词组，能够匹配上的可能少于1000个文档。但是如果查询 <code>the quick brown fox</code> 词组，几乎需要对索引中的100万个文档进行评分和排序，只是为了返回前 10 名最相关的文档。</p>
</div>
<div class="paragraph">
<p>问题的关键是 <code>the quick brown fox</code> 词组实际是查询 <code>the</code> 或 <code>quick</code> 或 <code>brown</code> 或 <code>fox</code>&#x2014; 任何文档即使它什么内容都没有而只包含 <code>the</code> 这个词也会被包括在结果集中。因此，我们需要找到一种降低待评分文档数量的方法。</p>
</div>
<div class="sect3">
<h4 id="stopwords-and">and 操作符 (and Operator)</h4>
<div class="paragraph">
<p>我们想要减少待评分文档的数量，最简单的方式就是在<a href="#match-improving-precision"><code>and</code> 操作符</a> <code>match</code> 查询时使用 <code>and</code> 操作符，这样可以让所有词都是必须的。</p>
</div>
<div class="paragraph">
<p>以下是 <code>match</code> 查询：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "match": {
        "text": {
            "query":    "the quick brown fox",
            "operator": "and"
             }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>上述查询被重写为 <code>bool</code> 查询如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "bool": {
        "must": [
            { "term": { "text": "the" }},
            { "term": { "text": "quick" }},
            { "term": { "text": "brown" }},
            { "term": { "text": "fox" }}
        ]
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>bool</code> 查询会智能的根据较优的顺序依次执行每个 <code>term</code> 查询：它会从最低频的词开始。因为所有词项都必须匹配，只要包含低频词的文档才有可能匹配。使用 <code>and</code> 操作符可以大大提升多词查询的速度。</p>
</div>
</div>
<div class="sect3">
<h4 id="_最少匹配数_minimum_should_match">最少匹配数(minimum_should_match)</h4>
<div class="paragraph">
<p>在精度匹配<a href="#match-precision">[match-precision]</a>的章节里面，我们讨论过使用 <code>minimum_should_match</code> 配置去掉结果中次相关的长尾。虽然它只对这个目的奏效，但是也为我们从侧面带来一个好处，它提供 <code>and</code> 操作符相似的性能。</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "match": {
        "text": {
            "query": "the quick brown fox",
            "minimum_should_match": "75%"
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>在上面这个示例中，四分之三的词都必须匹配，这意味着我们只需考虑那些包含最低频或次低频词的文档。
相比默认使用 <code>or</code> 操作符的简单查询，这为我们带来了巨大的性能提升。不过我们有办法可以做得更好……</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="common-terms">词项的分别管理</h3>
<div class="paragraph">
<p>在查询字符串中的词项可以分为更重要（低频词）和次重要（高频词）这两类。 只与次重要词项匹配的文档很有可能不太相关。实际上，我们想要文档能尽可能多的匹配那些更重要的词项。</p>
</div>
<div class="paragraph">
<p><code>match</code> 查询接受一个参数  <code>cutoff_frequency</code> ，从而可以让它将查询字符串里的词项分为低频和高频两组。低频组（更重要的词项）组成 <code>bulk</code> 大量查询条件，而高频组（次重要的词项）只会用来评分，而不参与匹配过程。通过对这两组词的区分处理，我们可以在之前慢查询的基础上获得巨大的速度提升。</p>
</div>
<div class="paragraph">
<p>领域相关的停用词（Domain-Specific Stopwords）</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p><code>cutoff_frequency</code> 配置的好处是，你在 <em>特定领域</em> 使用停用词不受约束。例如，关于电影网站使用的词 <em>movie</em> 、 <em>color</em> 、 <em>black</em> 和 <em>white</em> ，这些词我们往往认为几乎没有任何意义。使用 <code>stop</code> 词汇单元过滤器，这些特定领域的词必须手动添加到停用词列表中。然而 <code>cutoff_frequency</code> 会查看索引里词项的具体频率，这些词会被自动归类为 <em>高频词汇</em> 。</p>
</div>
</div>
</div>
<div class="paragraph">
<p>以下面查询为例：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "match": {
    "text": {
      "query": "Quick and the dead",
      "cutoff_frequency": 0.01 <b class="conum">(1)</b>
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>任何词项出现在文档中超过1%，被认为是高频词。<code>cutoff_frequency</code> 配置可以指定为一个分数（ <code>0.01</code> ）或者一个正整数（ <code>5</code> ）。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>此查询通过 <code>cutoff_frequency</code> 配置，将查询条件划分为低频组（ <code>quick</code> , <code>dead</code> ）和高频组（ <code>and</code> , <code>the</code> ）。然后，此查询会被重写为以下的 <code>bool</code> 查询：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "bool": {
    "must": { <b class="conum">(1)</b>
      "bool": {
        "should": [
          { "term": { "text": "quick" }},
          { "term": { "text": "dead"  }}
        ]
      }
    },
    "should": { <b class="conum">(2)</b>
      "bool": {
        "should": [
          { "term": { "text": "and" }},
          { "term": { "text": "the" }}
        ]
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>必须匹配至少一个低频／更重要的词项。</p>
</li>
<li>
<p>高频/次重要性词项是非必须的。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><code>must</code> 意味着至少有一个低频词&#x2014; <code>quick</code> 或者 <code>dead</code> &#x2014;必须出现在被匹配文档中。所有其他的文档被排除在外。 <code>should</code> 语句查找高频词 <code>and</code> 和 <code>the</code> ，但也只是在 <code>must</code> 语句查询的结果集文档中查询。
 <code>should</code> 语句的唯一的工作就是在对如 <code>Quick <em>and the</em> dead</code> 和 <code><em>The</em> quick but　dead</code> 语句进行评分时，前者得分比后者高。这种方式可以大大减少需要进行评分计算的文档数量。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>将操作符参数设置成 <code>and</code> 会要求所有低频词都必须匹配，同时对包含所有高频词的文档给予更高评分。但是，在匹配文档时，并不要求文档必须包含所有高频词。如果希望文档包含所有的低频和高频词，我们应该使用一个 <code>bool</code> 来替代。正如我们在<a href="#stopwords-and">and 操作符 (and Operator)</a>中看到的，它的查询效率已经很高了。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_控制精度">控制精度</h4>
<div class="paragraph">
<p><code>minimum_should_match</code> 参数可以与 <code>cutoff_frequency</code> 组合使用，但是此参数仅适用与低频词。如以下查询：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "match": {
    "text": {
      "query": "Quick and the dead",
      "cutoff_frequency": 0.01,
      "minimum_should_match": "75%"
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>将被重写为如下所示:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "bool": {
    "must": {
      "bool": {
        "should": [
          { "term": { "text": "quick" }},
          { "term": { "text": "dead"  }}
        ],
        "minimum_should_match": 1 <b class="conum">(1)</b>
      }
    },
    "should": { <b class="conum">(2)</b>
      "bool": {
        "should": [
          { "term": { "text": "and" }},
          { "term": { "text": "the" }}
        ]
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>&lt;1&gt;　因为只有两个词，原来的75%向下取整为 <code>1</code> ，意思是：必须匹配低频词的两者之一。
&lt;2&gt;　高频词仍可选的，并且仅用于评分使用。</p>
</div>
</div>
<div class="sect3">
<h4 id="_高频词">高频词</h4>
<div class="paragraph">
<p>当使用 <code>or</code> 查询高频词条，如&#x2014; <code>To be, or not to be</code> &#x2014;进行查询时性能最差。只是为了返回最匹配的前十个结果就对只是包含这些词的所有文档进行评分是盲目的。我们真正的意图是查询整个词条出现的文档，所以在这种情况下，不存低频所言，这个查询需要重写为所有高频词条都必须：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "bool": {
    "must": [
      { "term": { "text": "to" }},
      { "term": { "text": "be" }},
      { "term": { "text": "or" }},
      { "term": { "text": "not" }},
      { "term": { "text": "to" }},
      { "term": { "text": "be" }}
    ]
  }
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_对常用词使用更多控制_more_control_with_common_terms">对常用词使用更多控制（More Control with Common Terms）</h4>
<div class="paragraph">
<p>尽管高频/低频的功能在 <code>match</code> 查询中是有用的，有时我们还希望能对它有更多的控制，想控制它对高频和低频词分组的行为。　<code>match</code> 查询针对  <code>common</code> 词项查询提供了一组功能。</p>
</div>
<div class="paragraph">
<p>例如，我们可以让所有低频词都必须匹配，而只对那些包括超过 75% 的高频词文档进行评分：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "common": {
    "text": {
      "query":                  "Quick and the dead",
      "cutoff_frequency":       0.01,
      "low_freq_operator":      "and",
      "minimum_should_match": {
        "high_freq":            "75%"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>更多配置项参见　{ref}/query-dsl-common-terms-query.html[<code>common</code> terms query]。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="stopwords-phrases">停用词与短语查询</h3>
<div class="paragraph">
<p>所有查询中   <a href="#phrase-matching">[phrase-matching]</a> 大约占到5%，但是在慢查询里面它们又占大部分。
短语查询性能相对较差，特别是当短语中包括常用词的时候，如 <code>“To be, or not to be”</code> 短语全部由停用词组成，这是一种极端情况。原因在于几乎需要匹配全量的数据。</p>
</div>
<div class="paragraph">
<p>在 停用词的两面 <a href="#pros-cons-stopwords">停用词的优缺点</a>,中，我们提到移除停用词只能节省倒排索引中的一小部分空间。这句话只部分正确，一个典型的索引会可能包含部分或所有以下数据：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">词项字典（Terms dictionary）</dt>
<dd>
<p>索引中所有文档内所有词项的有序列表，以及包含该词的文档数量。</p>
</dd>
<dt class="hdlist1">倒排表（Postings list）</dt>
<dd>
<p>包含每个词项的文档（ID）列表。</p>
</dd>
<dt class="hdlist1">词频（Term frequency）</dt>
<dd>
<p>每个词项在每个文档里出现的频率。</p>
</dd>
<dt class="hdlist1">位置（Positions）</dt>
<dd>
<p>每个词项在每个文档里出现的位置，供短语查询或近似查询使用。</p>
</dd>
<dt class="hdlist1">偏移（Offsets）</dt>
<dd>
<p>每个词项在每个文档里开始与结束字符的偏移，供词语高亮使用，默认是禁用的。</p>
</dd>
<dt class="hdlist1">规范因子（Norms）</dt>
<dd>
<p>用来对字段长度进行规范化处理的因子，给较短字段予以更多权重。</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>将停用词从索引中移除会节省 <em>词项字典</em> 和 <em>倒排表</em> 里的少量空间，但 <em>位置</em> 和 <em>偏移</em> 是另一码事。位置和偏移数据很容易变成索引大小的两倍、三倍、甚至四倍。</p>
</div>
<div class="sect3">
<h4 id="_位置信息">位置信息</h4>
<div class="paragraph">
<p><code>analyzed</code> 字符串字段的位置信息默认是开启的， 所以短语查询能随时使用到它。
词项出现的越频繁，用来存储它位置信息的空间就越多。在一个大的文档集合中，对于那些非常常见的词，它们的位置信息可能占用成百上千兆的空间。</p>
</div>
<div class="paragraph">
<p>运行一个针对高频词 <code>the</code> 的短语查询可能会导致从磁盘读取好几G的数据。这些数据会被存储到内核文件系统的缓存中，以提高后续访问的速度，这看似是件好事，但这可能会导致其他数据从缓存中被剔除，进一步使后续查询变慢。</p>
</div>
<div class="paragraph">
<p>这显然是我们需要解决的问题。</p>
</div>
</div>
<div class="sect3">
<h4 id="index-options">索引选项</h4>
<div class="paragraph">
<p>我们首先应该问自己：是否真的需要使用短语查询或 近似查询？</p>
</div>
<div class="paragraph">
<p>答案通常是：不需要。在很多应用场景下，比如说日志，我们需要知道一个词 <em>是否</em> 在文档中（这个信息由倒排表提供）而不是关心词的位置在哪里。或许我们要对一两个字段使用短语查询，但是我们完全可以在其他 <code>analyzed</code> 字符串字段上禁用位置信息。</p>
</div>
<div class="paragraph">
<p><code>index_options</code> 参数  允许我们控制索引里为每个字段存储的信息。 可选值如下:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>docs</code></dt>
<dd>
<p>只存储文档及其包含词项的信息。这对 <code>not_analyzed</code> 字符串字段是默认的。</p>
</dd>
<dt class="hdlist1"><code>freqs</code></dt>
<dd>
<p>存储 <code>docs</code> 信息，以及每个词在每个文档里出现的频次。词频是完成<a href="#relevance-intro">TF/IDF</a> 相关度计算的必要条件，但如果只想知道一个文档是否包含某个特定词项，则无需使用它。</p>
</dd>
<dt class="hdlist1"><code>positions</code></dt>
<dd>
<p>存储 <code>docs</code> 、 <code>freqs</code> 、 <code>analyzed</code> ，以及每个词项在每个文档里出现的位置。 这对 <code>analyzed</code> 字符串字段是默认的，但当不需使用短语或近似匹配时，可以将其禁用。</p>
</dd>
<dt class="hdlist1"><code>offsets</code></dt>
<dd>
<p>存储 <code>docs</code>,<code>freqs</code>,<code>positions</code>, 以及每个词在原始字符串中开始与结束字符的偏移信息( {ref}/search-request-highlighting.html#postings-highlighter[<code>postings</code> highlighter] )。这个信息被用以高亮搜索结果，但它默认是禁用的。</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>我们可以在索引创建的时候为字段设置 <code>index_options</code> 选项，或者在使用  <code>put-mapping</code> API新增字段映射的时候设置。我们无法修改已有字段的这个设置：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "title": { <b class="conum">(1)</b>
          "type":          "string"
       },
        "content": { <b class="conum">(2)</b>
          "type":          "string",
          "index_options": "freqs"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>title</code> 字段使用默认的 <code>positions</code> 设置，所以它适于短语或近似查询。</p>
</li>
<li>
<p><code>content</code> 字段的位置设置是禁用的，所以它无法用于短语或近似查询。</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_停用词">停用词</h4>
<div class="paragraph">
<p>删除停用词是能显著降低位置信息所占空间的一种方式。    一个被删除停用词的索引仍然可以使用短语查询，因为剩下的词的原始位置仍然被保存着，这正如 <a href="#maintaining-positions">保持位置（Maintaining Positions）</a> 中看到的那样。 尽管如此，将词项从索引中排除终究会降低搜索能力，这使我们难以区分  <em>Man in the moon</em> 与  <em>Man on the moon</em> 这两个短语。</p>
</div>
<div class="paragraph">
<p>幸运的是，鱼与熊掌是可以兼得的：请查看 <a href="#common-grams"><code>common_grams</code> 过滤器</a>。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="common-grams">common_grams 过滤器</h3>
<div class="paragraph">
<p><code>common_grams</code> 过滤器是针对短语查询能更高效的使用停用词而设计的。 它与 shingles 过滤器类似（参见 查找相关词（<a href="#shingles">[shingles]</a>)), 为每个相邻词对生成 ，用示例解释更为容易。</p>
</div>
<div class="paragraph">
<p><code>common_grams</code> 过滤器根据 <code>query_mode</code> 设置的不同而生成不同输出结果：<code>false</code> （为索引使用） 或 <code>true</code> （为搜索使用），所以我们必须创建两个独立的分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "index_filter": { <b class="conum">(1)</b>
          "type":         "common_grams",
          "common_words": "_english_" <b class="conum">(2)</b>
        },
        "search_filter": { <b class="conum">(1)</b>
          "type":         "common_grams",
          "common_words": "_english_", <b class="conum">(2)</b>
          "query_mode":   true
        }
      },
      "analyzer": {
        "index_grams": { <b class="conum">(3)</b>
          "tokenizer":  "standard",
          "filter":   [ "lowercase", "index_filter" ]
        },
        "search_grams": { <b class="conum">(3)</b>
          "tokenizer": "standard",
          "filter":  [ "lowercase", "search_filter" ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>首先我们基于 <code>common_grams</code> 过滤器创建两个过滤器：
<code>index_filter</code> 在索引时使用（此时 <code>query_mode</code>  的默认设置是 <code>false</code> ）， <code>search_filter</code> 在查询时使用（此时 <code>query_mode</code> 的默认设置是 <code>true</code> ）。</p>
</li>
<li>
<p><code>common_words</code> 参数可以接受与 <code>stopwords</code> 参数同样的选项（参见 指定停用词 <a href="#specifying-stopwords">指定停用词（Specifying Stopwords）</a> ）。这个过滤器还可以接受参数 <code>common_words_path</code> ，使用存于文件里的常用词。</p>
</li>
<li>
<p>然后我们使用过滤器各创建一个索引时分析器和查询时分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>有了自定义分析器，我们可以创建一个字段在索引时使用 <code>index_grams</code> 分析器：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index/_mapping/my_type
{
  "properties": {
    "text": {
      "type":            "string",
      "analyzer":  "index_grams", <b class="conum">(1)</b>
      "search_analyzer": "standard" <b class="conum">(1)</b>
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>text</code> 字段索引时使用 <code>index_grams</code> 分析器，但是在搜索时默认使用 <code>standard</code> 分析器，稍后我们会解释其原因。</p>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="_索引时_at_index_time">索引时（At Index Time）</h4>
<div class="paragraph">
<p>如果我们对短语  <em>The quick and brown fox</em> 进行拆分，它生成如下词项：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: the_quick
Pos 2: quick_and
Pos 3: and_brown
Pos 4: brown_fox</code></pre>
</div>
</div>
<div class="paragraph">
<p>新的 <code>index_grams</code> 分析器生成以下词项：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: the, the_quick
Pos 2: quick, quick_and
Pos 3: and, and_brown
Pos 4: brown
Pos 5: fox</code></pre>
</div>
</div>
<div class="paragraph">
<p>所有的词项都是以 <code>unigrams</code> 形式输出的（the、quick 等等），但是如果一个词本身是常用词或者跟随着常用词，那么它同时还会在 <code>unigram</code> 同样的位置以 <code>bigram</code> 形式输出：<code>the_quick</code> ， <code>quick_and</code> ， <code>and_brown</code> 。</p>
</div>
</div>
<div class="sect3">
<h4 id="_单字查询_unigram_queries">单字查询（Unigram Queries）</h4>
<div class="paragraph">
<p>因为索引包含 <code>unigrams</code> ，可以使用与其他字段相同的技术进行查询，例如：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_search
{
  "query": {
    "match": {
      "text": {
        "query": "the quick and brown fox",
        "cutoff_frequency": 0.01
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>上面这个查询字符串是通过为文本字段配置的 <code>search_analyzer</code>  分析器 --本例中使用的是 <code>standard</code> 分析器-- 进行分析的， 它生成的词项为： <code>the</code> ， <code>quick</code> ， <code>and</code> ， <code>brown</code> ， <code>fox</code> 。</p>
</div>
<div class="paragraph">
<p>因为 <code>text</code> 字段的索引中包含与 <code>standard</code> 分析去生成的一样的 <code>unigrams</code> ，搜索对于任何普通字段都能正常工作。</p>
</div>
</div>
<div class="sect3">
<h4 id="_二元语法短语查询_bigram_phrase_queries">二元语法短语查询（Bigram Phrase Queries）</h4>
<div class="paragraph">
<p>但是，当我们进行短语查询时，我们可以用专门的 <code>search_grams</code>  分析器让整个过程变得更高效：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_search
{
  "query": {
    "match_phrase": {
      "text": {
        "query":    "The quick and brown fox",
        "analyzer": "search_grams" <b class="conum">(1)</b>
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>对于短语查询，我们重写了默认的 <code>search_analyzer</code> 分析器，而使用 <code>search_grams</code> 分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><code>search_grams</code> 分析器会生成以下词项：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: the_quick
Pos 2: quick_and
Pos 3: and_brown
Pos 4: brown
Pos 5: fox</code></pre>
</div>
</div>
<div class="paragraph">
<p>分析器排除了所有常用词的 <code>unigrams</code>，只留下常用词的 <code>bigrams</code> 以及低频的 <code>unigrams</code>。如 <code>the_quick</code> 这样的 <code>bigrams</code> 比单个词项 <code>the</code> 更为少见，这样有两个好处：</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>the_quick</code> 的位置信息要比 <code>the</code> 的小得多，所以它读取磁盘更快，对系统缓存的影响也更小。</p>
</li>
<li>
<p>词项 <code>the_quick</code> 没有 <code>the</code> 那么常见，所以它可以大量减少需要计算的文档。</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_两词短语_two_word_phrases">两词短语（Two-Word Phrases）</h4>
<div class="paragraph">
<p>我们的优化可以更进一步，因为大多数的短语查询只由两个词组成，如果其中一个恰好又是常用词，例如：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_search
{
  "query": {
    "match_phrase": {
      "text": {
        "query":    "The quick",
        "analyzer": "search_grams"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>那么 <code>search_grams</code> 分析器会输出单个语汇单元：<code>the_quick</code> 。这将原来昂贵的查询（查询 <code>the</code> 和 <code>quick</code> ）转换成了对单个词项的高效查找。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="stopwords-relavance">停用词与相关性</h3>
<div class="paragraph">
<p>在结束停用词相关内容之前，最后一个话题是关于相关性的。在索引中保留停用词会降低相关度计算的准确性，特别是当我们的文档非常长时。</p>
</div>
<div class="paragraph">
<p>正如我们在 <a href="#bm25-saturation">[bm25-saturation]</a> 已经讨论过的，
原因在于 <a href="#bm25-saturation">[bm25-saturation]</a> 并没有强制对词频率的影响设置上限 。 基于逆文档频率的影响，非常常用的词可能只有很低的权重，但是在长文档中，单个文档出现的绝对数量很大的停用词会导致这些词被不自然的加权。</p>
</div>
<div class="paragraph">
<p>可以考虑对包含停用词的较长字段使用 <a href="#bm25">Okapi BM25</a> 相似度算法，而不是默认的 Lucene 相似度。</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="synonyms">同义词</h2>
<div class="sectionbody">
<div class="paragraph">
<p>词干提取是通过简化他们的词根形式来扩大搜索的范围，同义词  通过相关的观念和概念来扩大搜索范围。
也许没有文档匹配查询 “英国女王“ ，但是包含 “英国君主” 的文档可能会被认为是很好的匹配。</p>
</div>
<div class="paragraph">
<p>用户搜索 “美国” 并且期望找到包含 <em>美利坚合众国</em> 、 <em>美国</em> 、 <em>美洲</em> 、或者 <em>美国各州</em> 的文档。
然而，他们不希望搜索到关于 <code>国事</code> 或者 <code>政府机构</code> 的结果。</p>
</div>
<div class="paragraph">
<p>这个例子提供了宝贵的经验，它向我们阐述了，区分不同的概念对于人类是多么简单而对于纯粹的机器是多么棘手的事情。通常我们会对语言中的每一个词去尝试提供同义词以确保任何一个文档都是可发现的，以保证不管文档之间有多么微小的关联性都能够被检索出来。</p>
</div>
<div class="paragraph">
<p>这样做是不对的。就像我们更喜欢不用或少用词根而不是过分使用词根一样，同义词也应该只在必要的时候使用。
这是因为用户可以理解他们的搜索结果受限于他们的搜索词，如果搜索结果看上去几乎是随机时，他们就会变得无法理解（注：大规模使用同义词会导致查询结果趋向于让人觉得是随机的）。</p>
</div>
<div class="paragraph">
<p>同义词可以用来合并几乎相同含义的词，如 <code>跳</code> 、 <code>跳越</code> 或者 <code>单脚跳行</code> ，和 <code>小册子</code> 、 <code>传单</code> 或者 <code>资料手册</code> 。
或者，它们可以用来让一个词变得更通用。例如， <code>鸟</code> 可以作为 <code>猫头鹰</code> 或 <code>鸽子</code> 的通用代名词，还有， <code>成人</code> 可以被用于 <code>男人</code> 或者 <code>女人</code> 。</p>
</div>
<div class="paragraph">
<p>同义词似乎是一个简单的概念，但是正确的使用它们却是非常困难的。在这一章，我们会介绍使用同义词的技巧和讨论它的局限性和陷阱。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>同义词扩大了一个匹配文件的范围。正如 <a href="#stemming">词干提取</a> 或者 <a href="#partial-matching">部分匹配</a> ，同义词的字段不应该被单独使用，而应该与一个针对主字段的查询操作一起使用，这个主字段应该包含纯净格式的原始文本。
在使用同义词时，参阅 <a href="#most-fields">[most-fields]</a> 的解释来维护相关性。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="using-synonyms">使用同义词</h3>
<div class="paragraph">
<p>同义词可以取代现有的语汇单元或  通过使用  {ref}/analysis-synonym-tokenfilter.html[ <code>同义词</code> 语汇单元过滤器]，添加到语汇单元流中：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym_filter": {
          "type": "synonym", <b class="conum">(1)</b>
          "synonyms": [ <b class="conum">(2)</b>
            "british,english",
            "queen,monarch"
          ]
        }
      },
      "analyzer": {
        "my_synonyms": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_synonym_filter" <b class="conum">(3)</b>
          ]
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>首先，我们定义了一个 <code>同义词</code> 类型的语汇单元过滤器。</p>
</li>
<li>
<p>我们在 <a href="#synonym-formats">同义词格式</a> 中讨论同义词格式。</p>
</li>
<li>
<p>然后我们创建了一个使用 <code>my_synonym_filter</code> 的自定义分析器。</p>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>同义词可以使用 <code>synonym</code> 参数来内嵌指定，或者必须  存在于集群每一个节点上的同义词文件中。
同义词文件路径由 <code>synonyms_path</code> 参数指定，应绝对或相对于 Elasticsearch <code>config</code> 目录。参照 <a href="#updating-stopwords">更新停用词（Updating Stopwords）</a> 的技巧，可以用来刷新的同义词列表。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>通过 <code>analyze</code> API 来测试我们的分析器，显示如下：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_analyze?analyzer=my_synonyms
Elizabeth is the English queen</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1: (elizabeth)
Pos 2: (is)
Pos 3: (the)
Pos 4: (british,english) <b class="conum">(1)</b>
Pos 5: (queen,monarch) <b class="conum">(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>所有同义词与原始词项占有同一个位置。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>这样的一个文件将匹配任何以下的查询： <code>English queen</code> 、<code>British queen</code> 、 <code>English monarch</code> 或 <code>British monarch</code> 。
即使是一个短语查询也将会工作，因为每个词项的位置已被保存。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>在索引和搜索中使用相同的同义词语汇单元过滤器是多余的。  如果在索引的时候，我们用 <code>english</code> 和 <code>british</code> 这两个术语代替 <code>English</code> ，
然后在搜索的时候，我们只需要搜索这些词项中的一个。或者，如果在索引的时候我们不使用同义词，然后在搜索的时候，我们将需要把对 <code>English</code> 的查询转换为 <code>english</code> 或者 <code>british</code> 的查询。</p>
</div>
<div class="paragraph">
<p>是否在搜索或索引的时候做同义词扩展可能是一个困难的选择。我们将探索更多的选择 <a href="#synonyms-expand-or-contract">扩展或收缩</a>。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="synonym-formats">同义词格式</h3>
<div class="paragraph">
<p>同义词最简单的表达形式是  逗号分隔：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"jump,leap,hop"</pre>
</div>
</div>
<div class="paragraph">
<p>如果遇到这些词项中的任何一项，则将其替换为所有列出的同义词。例如：</p>
</div>
<div class="listingblock pagebreak-before">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">原始词项:   取代:
────────────────────────────────
jump            → (jump,leap,hop)
leap            → (jump,leap,hop)
hop             → (jump,leap,hop)</code></pre>
</div>
</div>
<div class="paragraph">
<p>或者, 使用 <code>&#8658;</code> 语法，可以指定一个词项列表（在左边），和一个或多个替换（右边）的列表：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"u s a,united states,united states of america =&gt; usa"
"g b,gb,great britain =&gt; britain,england,scotland,wales"</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">原始词项:   取代:
────────────────────────────────
u s a           → (usa)
united states   → (usa)
great britain   → (britain,england,scotland,wales)</code></pre>
</div>
</div>
<div class="paragraph">
<p>如果多个规则指定同一个同义词，它们将被合并在一起，且顺序无关，否则使用最长匹配。以下面的规则为例：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"united states            =&gt; usa",
"united states of america =&gt; usa"</pre>
</div>
</div>
<div class="paragraph">
<p>如果这些规则相互冲突，Elasticsearch 会将 <code>United States of America</code> 转换为词项 <code>(usa),(of),(america)</code> 。否则，会使用最长的序列，即最终得到词项 <code>(usa)</code> 。</p>
</div>
</div>
<div class="sect2">
<h3 id="synonyms-expand-or-contract">扩展或收缩</h3>
<div class="paragraph">
<p>在 <a href="#synonym-formats">同义词格式</a> 中，我们看到了可以通过 <em>简单扩展</em> 、 <em>简单收缩</em> 、或_类型扩展_ 来指明同义词规则。
本章节我们将在这三者间做个权衡比较。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
本节仅处理单词同义词。多词同义词又增添了一层复杂性，在 <a href="#multi-word-synonyms">多词同义词和短语查询</a> 中，我们将会讨论。
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="synonyms-expansion">简单扩展</h4>
<div class="paragraph">
<p>通过 <em>简单扩展</em> ，我们可以把同义词列表中的任意一个词扩展成同义词列表 <em>所有</em> 的词：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"jump,hop,leap"</pre>
</div>
</div>
<div class="paragraph">
<p>扩展可以应用在索引阶段或查询阶段。两者都有优点
(⬆)︎ 和缺点 (⬇)︎。到底要在哪个阶段使用，则取决于性能与灵活性：</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">索引</th>
<th class="tableblock halign-left valign-top">查询</th>
</tr>
</thead>
<tbody>
<tr>
<th class="tableblock halign-left valign-top"><p class="tableblock">索引的大小</p></th>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬇︎ 大索引。因为所有的同义词都会被索引，所以索引的大小相对会变大一些。</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬆︎ 正常大小。</p></td>
</tr>
<tr>
<th class="tableblock halign-left valign-top"><p class="tableblock">关联</p></th>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬇︎ 所有同义词都有相同的 IDF（至于什么是 IDF ，参见 <a href="#relevance-intro">[relevance-intro]</a>），这意味着通用的词和较常用的词都拥有着相同的权重。</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬆︎ 每个同义词 IDF 都和原来一样。</p></td>
</tr>
<tr>
<th class="tableblock halign-left valign-top"><p class="tableblock">性能</p></th>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬆︎ 查询只需要找到查询字符串中指定单个词项。</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬇︎ 对一个词项的查询重写来查找所有的同义词，从而降低性能。</p></td>
</tr>
<tr>
<th class="tableblock halign-left valign-top"><p class="tableblock">灵活性</p></th>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬇︎ 同义词规则不能改变现有的文件。对于有影响的新规则，现有的文件都要重建（注：重新索引一次文档）。</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">⬆︎ 同义词规则可以更新不需要索引文件。</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="synonyms-contraction">简单收缩</h4>
<div class="paragraph">
<p><em>简单收缩</em> ，把  左边的多个同义词映射到了右边的单个词：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"leap,hop =&gt; jump"</pre>
</div>
</div>
<div class="paragraph">
<p>它必须同时应用于索引和查询阶段，以确保查询词项映射到索引中存在的同一个值。</p>
</div>
<div class="paragraph">
<p>相对于简单扩展方法，这种方法也有一些优点和一些缺点：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">索引的大小</dt>
<dd>
<p>⬆︎ 索引大小是正常的，因为只有单一词项被索引。</p>
</dd>
<dt class="hdlist1">关联</dt>
<dd>
<p>⬇︎ 所有词项的 IDF 是一样的，所以你不能区分比较常用的词、不常用的单词。</p>
</dd>
<dt class="hdlist1">性能</dt>
<dd>
<p>⬆︎ 查询只需要在索引中找到单词的出现。</p>
</dd>
<dt class="hdlist1">灵活性</dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>⬆︎ 新同义词可以添加到规则的左侧并在查询阶段使用。例如，我们想添加 <code>bound</code> 到先前指定的同义词规则中。那么下面的规则将作用于包含 <code>bound</code> 的查询或包含 <code>bound</code> 的文档索引：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"leap,hop,bound =&gt; jump"</pre>
</div>
</div>
<div class="paragraph">
<p>似乎对旧有的文档不起作用是么？其实我们可以把上面这个同义词规则改写下，以便对旧有文档同样起作用：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"leap,hop,bound =&gt; jump,bound"</pre>
</div>
</div>
<div class="paragraph">
<p>当你重建索引文件，你可以恢复到上面的规则（注： <code>leap,hop,bound &#8658; jump</code> ）来获得查询单个词项的性能优势（注：因为上面那个规则相比这个而言，查询阶段就只要查询一个词了）。</p>
</div>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="synonyms-genres">类型扩展</h4>
<div class="paragraph">
<p>类型扩展是完全不同于简单收缩  或扩张，
并不是平等看待所有的同义词，而是扩大了词的意义，使被拓展的词更为通用。以这些规则为例：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"cat    =&gt; cat,pet",
"kitten =&gt; kitten,cat,pet",
"dog    =&gt; dog,pet"
"puppy  =&gt; puppy,dog,pet"</pre>
</div>
</div>
<div class="paragraph">
<p>通过在索引阶段使用类型扩展：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>一个关于 <code>kitten</code> 的查询会发现关于 kittens 的文档。</p>
</li>
<li>
<p>查询一个 <code>cat</code> 会找到关于 kittens 和 cats 的文档。</p>
</li>
<li>
<p>一个 <code>pet</code> 的查询将发现有关的 kittens、cats、puppies、dogs 或者 pets 的文档。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>或者在查询阶段使用类型扩展， <code>kitten</code> 的查询结果就会被拓展成涉及到 kittens、cats、dogs。</p>
</div>
<div class="paragraph">
<p>您也可以有两全其美的办法，通过在索引阶段应用类型扩展同义词规则，以确保类型在索引中存在。然后，在查询阶段，
你可以选择不采用同义词（使 <code>kitten</code> 查询只返回 kittens 的文件）或采用同义词， <code>kitten</code> 的查询操作就会返回包括 kittens、cats、pets（也包括 dogs 和 puppies）的相关结果。</p>
</div>
<div class="paragraph">
<p>前面的示例规则，对 <code>kitten</code> 的 IDF 将是正确的，而 <code>cat</code> 和 <code>pet</code> 的 IDF 将会被 Elasticsearch 降权。然而, 这是对你有利的，当一个针对 <code>kitten</code> 的查询被拓展成了针对 <code>kitten OR cat OR pet</code> 的查询， 那么 <code>kitten</code> 相关的文档就应该排在最上方，其次是 <code>cat</code> 的文件， <code>pet</code> 的文件将被排在最底部。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="synonyms-analysis-chain">同义词和分析链</h3>
<div class="paragraph">
<p>在  <a href="#synonym-formats">同义词格式</a> 一章中，我们使用 <code>u s a</code> 来举例阐述一些同义词相关的知识。那么为什么
我们使用的不是 <code>U.S.A.</code> 呢？原因是， 这个 <code>同义词</code> 的语汇单元过滤器只能接收到在它前面的语汇单元过滤器或者分词器的输出结果（这里看不到原始文本）。</p>
</div>
<div class="paragraph">
<p>假设我们有一个分析器，它由 <code>standard</code> 分词器、 <code>lowercase</code> 的语汇单元过滤器、 <code>synonym</code> 的语汇单元过滤器组成。文本 <code>U.S.A.</code> 的分析过程，看起来像这样的：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">original string（原始文本）                       → "U.S.A."
standard           tokenizer（分词器）            → (U),(S),(A)
lowercase          token filter（语汇单元过滤器）  → (u),(s),(a)
synonym            token filter（语汇单元过滤器）  → (usa)</code></pre>
</div>
</div>
<div class="paragraph">
<p>如果我们有指定的同义词 <code>U.S.A.</code> ，它永远不会匹配任何东西。因为， <code>my_synonym_filter</code> 看到词项的时候，句号已经被移除了，并且字母已经被小写了。</p>
</div>
<div class="paragraph">
<p>这其实是一个非常需要注意的地方。如果我们想同时使用同义词特性与词根提取特性，那么 <code>jumps</code> 、 <code>jumped</code> 、 <code>jump</code> 、 <code>leaps</code> 、 <code>leaped</code> 和 <code>leap</code> 这些词是否都会被索引成一个 <code>jump</code> ？
我们  可以把同义词过滤器放置在词根提取之前，然后把所有同义词以及词形变化都列举出来：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"jumps,jumped,leap,leaps,leaped =&gt; jump"</pre>
</div>
</div>
<div class="paragraph">
<p>但更简洁的方式将同义词过滤器放置在词根过滤器之后，然后把词根形式的同义词列举出来：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"leap =&gt; jump"</pre>
</div>
</div>
<div class="sect3">
<h4 id="_大小写敏感的同义词">大小写敏感的同义词</h4>
<div class="paragraph">
<p>通常，我们把同义词过滤器放置在 <code>lowercase</code> 语汇单元过滤器之后，因此，所有的同义词  都是小写。
但有时会导致奇怪的合并。例如， <code>CAT</code> 扫描和一只 <code>cat</code> 有很大的不同，或者 <code>PET</code> （正电子发射断层扫描）和 <code>pet</code> 。
就此而言，姓 <code>Little</code> 也是不同于形容词 <code>little</code> 的 (尽管当一个句子以它开头时，首字母会被大写)。</p>
</div>
<div class="paragraph">
<p>如果根据使用情况来区分词义，则需要将同义词过滤器放置在 <code>lowercase</code> 筛选器之前。当然，这意味着同义词规则需要列出所有想匹配的变化（例如， <code>Little、LITTLE、little</code> ）。</p>
</div>
<div class="paragraph">
<p>相反,可以有两个同义词过滤器：一个匹配大小写敏感的同义词，一个匹配大小写不敏感的同义词。例如，大小写敏感的同义词规则可以是这个样子：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"CAT,CAT scan           =&gt; cat_scan"
"PET,PET scan           =&gt; pet_scan"
"Johnny Little,J Little =&gt; johnny_little"
"Johnny Small,J Small   =&gt; johnny_small"</pre>
</div>
</div>
<div class="paragraph">
<p>大小不敏感的同义词规则可以是这个样子：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"cat                    =&gt; cat,pet"
"dog                    =&gt; dog,pet"
"cat scan,cat_scan scan =&gt; cat_scan"
"pet scan,pet_scan scan =&gt; pet_scan"
"little,small"</pre>
</div>
</div>
<div class="paragraph">
<p>大小写敏感的同义词规则不仅会处理 <code>CAT scan</code> ，而且有时候也可能会匹配到 <code>CAT scan</code> 中的 <code>CAT</code> （注：从而导致 <code>CAT scan</code> 被转化成了同义词 <code>cat_scan scan</code> ）。出于这个原因，在大小写敏感的同义词列表中会有一个针对较坏替换情况的特异规则 <code>cat_scan scan</code> 。</p>
</div>
<div class="paragraph">
<p>提示： 可以看到它们可以多么轻易地变得复杂。同平时一样， <code>analyze</code> API 是帮手，用它来检查分析器是否正确配置。参阅 <a href="#analyze-api">[analyze-api]</a>。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="multi-word-synonyms">多词同义词和短语查询</h3>
<div class="paragraph">
<p>至此，同义词看上去还挺简单的。然而不幸的是，复杂的部分才刚刚开始。 为了能使 <a href="#phrase-matching">短语查询</a> 正常工作，
Elasticsearch 需要知道每个词在初始文本中的位置。多词同义词会严重破坏词的位置信息，尤其当新增的同义词标记长度各不相同的时候。</p>
</div>
<div class="paragraph">
<p>我们创建一个同义词语汇单元过滤器，然后使用下面这样的同义词规则：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"usa,united states,u s a,united states of america"</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym_filter": {
          "type": "synonym",
          "synonyms": [
            "usa,united states,u s a,united states of america"
          ]
        }
      },
      "analyzer": {
        "my_synonyms": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_synonym_filter"
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_synonyms&amp;text=
The United States is wealthy</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>解析器</code> 会输出下面这样的结果：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1:  (the)
Pos 2:  (usa,united,u,united)
Pos 3:  (states,s,states)
Pos 4:  (is,a,of)
Pos 5:  (wealthy,america)</code></pre>
</div>
</div>
<div class="paragraph">
<p>如果你用上面这个同义词语汇单元过滤器索引一个文档，然后执行一个短语查询，那你就会得到惊人的结果，下面这些短语都不会匹配成功：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The usa is wealthy</p>
</li>
<li>
<p>The united states of america is wealthy</p>
</li>
<li>
<p>The U.S.A. is wealthy</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>但是这些短语会：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>United states is wealthy</p>
</li>
<li>
<p>Usa states of wealthy</p>
</li>
<li>
<p>The U.S. of wealthy</p>
</li>
<li>
<p>U.S. is america</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>如果你是在查询阶段使同义词，那你就会看到更加诡异的匹配结果。看下这个 <code>validate-query</code> 查询：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_validate/query?explain
{
  "query": {
    "match_phrase": {
      "text": {
        "query": "usa is wealthy",
        "analyzer": "my_synonyms"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>查询关键字会被同义词语汇单元过滤器处理成类似这样的信息：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"(usa united u united) (is states s states) (wealthy a of) america"</pre>
</div>
</div>
<div class="paragraph">
<p>这会匹配包含有 <code>u is of america</code> 的文档，但是匹配不出任何含有 <code>america</code> 的文档。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>多词同义对高亮匹配结果也会造成影响。一个针对 <code>USA</code> 的查询，返回的结果可能却高亮了： The <em>United States is wealthy</em>  。</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_使用简单收缩进行短语查询">使用简单收缩进行短语查询</h4>
<div class="paragraph">
<p>避免这种混乱的方法是使用 <a href="#synonyms-contraction">简单收缩</a>，
用单个词项表示所有的同义词，
然后在查询阶段，就只需要针对这单个词进行查询了：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym_filter": {
          "type": "synonym",
          "synonyms": [
            "united states,u s a,united states of america=&gt;usa"
          ]
        }
      },
      "analyzer": {
        "my_synonyms": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_synonym_filter"
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_synonyms
The United States is wealthy</code></pre>
</div>
</div>
<div class="paragraph">
<p>上面那个查询信息就会被处理成类似下面这样：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Pos 1:  (the)
Pos 2:  (usa)
Pos 3:  (is)
Pos 5:  (wealthy)</code></pre>
</div>
</div>
<div class="paragraph">
<p>现在我们再次执行我们之前做过的那个 <code>validate-query</code> 查询，就会输出一个简单又合理的结果：</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"usa is wealthy"</pre>
</div>
</div>
<div class="paragraph">
<p>这个方法的缺点是，因为把 <code>united states of america</code> 转换成了同义词 <code>usa</code>, 你就不能使用 <code>united states of america</code> 去搜索出 <code>united</code> 或者 <code>states</code> 。
你需要使用一个额外的字段并用另一个解析器链来达到这个目的。</p>
</div>
</div>
<div class="sect3">
<h4 id="_同义词与_query_string_查询">同义词与 query_string 查询</h4>
<div class="paragraph">
<p>本书很少谈论到 <code>query_string</code> 查询，因为真心不推荐你用它。
在 <a href="#query-string-query">复杂查询</a> 一节中有提到，由于 <code>query_string</code> 查询支持一个精简的 <em>查询语法</em> ，因此，可能这会导致它搜出一些出人意料的结果或者甚至是含有语法错误的结果。</p>
</div>
<div class="paragraph">
<p>这种查询方式存在不少问题，而其中之一便与多词同义有关。为了支持它的查询语法，你必须用指定的、该语法所能识别的操作符号来标示，比如 <code>AND</code> 、 <code>OR</code> 、 <code>+</code> 、 <code>-</code> 、 <code>field:</code> 等等。
(更多相关内容参阅 {ref}/query-dsl-query-string-query.html#query-string-syntax[<code>query_string</code> 语法] 。)</p>
</div>
<div class="paragraph">
<p>而在这种语法的解析过程中，解析动作会把查询文本在空格符处作切分，然后分别把每个切分出来的词传递给相关性解析器。
这也即意味着你的同义词解析器永远都不可能收到类似 <code>United States</code> 这样的多个单词组成的同义词。由于不会把 <code>United States</code> 作为一个原子性的文本，所以同义词解析器的输入信息永远都是两个被切分开的词 <code>United</code> 和 <code>States</code> 。</p>
</div>
<div class="paragraph">
<p>所幸， <code>match</code> 查询相比而言就可靠得多了，因为它不支持上述语法，所以多个字组成的同义词不会被切分开，而是会完整地交给解析器处理。</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="symbol-synonyms">符号同义词</h3>
<div class="paragraph">
<p>最后一节内容我们来阐述下怎么对符号进行同义词处理，这和我们前面讲的同义词处理不太一样。
 <em>符号同义词</em> 是用别名来表示这个符号，以防止它在分词过程中被误认为是不重要的标点符号而被移除。</p>
</div>
<div class="paragraph">
<p>虽然绝大多数情况下，符号对于全文搜索而言都无关紧要，但是字符组合而成的表情，或许又会是很有意义的东西，甚至有时候会改变整个句子的含义，对比一下这两句话：</p>
</div>
<div class="ulist pagebreak-before">
<ul>
<li>
<p>我很高兴能在星期天工作。</p>
</li>
<li>
<p>我很高兴能在星期天工作 :( （注：难过的表情）</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><code>标准</code> （注：standard）分词器或许会简单地消除掉第二个句子里的字符表情，致使两个原本意思相去甚远的句子变得相同。</p>
</div>
<div class="paragraph">
<p>我们可以先使用
{ref}/analysis-mapping-charfilter.html[<code>映射</code>]字符过滤器，在文本被递交给分词器处理之前，
把字符表情替换成符号同义词 <code>emoticon_happy</code> 或者
<code>emoticon_sad</code> ：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": {
        "emoticons": {
          "type": "mapping",
          "mappings": [ <b class="conum">(1)</b>
            ":)=&gt;emoticon_happy",
            ":(=&gt;emoticon_sad"
          ]
        }
      },
      "analyzer": {
        "my_emoticons": {
          "char_filter": "emoticons",
          "tokenizer":   "standard",
          "filter":    [ "lowercase" ]
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze?analyzer=my_emoticons
I am :) not :( <b class="conum">(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>映射</code> 过滤器把字符从 <code>&#8658;</code> 左边的格式转变成右边的样子。</p>
</li>
<li>
<p>输出： <code>i</code> 、 <code>am</code> 、 <code>emoticon_happy</code> 、 <code>not</code> 、 <code>emoticon_sad</code> 。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>很少有人会搜 <code>emoticon_happy</code> 这个词，但是确保类似字符表情的这类重要符号被存储到索引中是非常好的做法，在进行情感分析的时候会很有用。当然，我们也可以用真实的词汇来处理符号同义词，比如： <code>happy</code> 或者 <code>sad</code> 。</p>
</div>
<div class="paragraph">
<p>提示： <code>映射</code> 字符过滤器是个非常有用的过滤器，它可以用来对一些已有的字词进行替换操作，
你如果想要采用更灵活的正则表达式去替换字词的话，那你可以使用
{ref}/analysis-pattern-replace-charfilter.html[ <code>pattern_replace</code> ]字符过滤器。</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="fuzzy-matching">拼写错误</h2>
<div class="sectionbody">
<div class="paragraph">
<p>我们期望在类似时间和价格的结构化数据上执行一个查询来返回精确匹配的文档。
然而，好的全文检索不应该是完全相同的限定逻辑。
相反，我们可以扩大范围以包括 <em>可能</em> 的匹配，而根据相关性得分将更好的匹配推到结果集的顶部。</p>
</div>
<div class="paragraph">
<p>事实上，只能完全匹配的全文搜索可能会困扰你的用户。
难道不希望在搜索 <code>quick brown fox</code> 时匹配一个包含  <code>fast brown foxes</code> 的文档，
搜索 <code>Johnny Walker</code> 同时匹配 <code>Johnnie Walker</code> ，搜索 <code>Arnold Shcwarzenneger</code> 同时匹配 <code>Arnold Schwarzenegger</code> ?</p>
</div>
<div class="paragraph">
<p>如果存在完全符合用户查询的文档，他们应该出现在结果集的顶部，而较弱的匹配可以被包含在列表的后面。
如果没有精确匹配的文档，至少我们可以显示有可能匹配用户要求的文档，它们甚至可能是用户最初想要的！</p>
</div>
<div class="paragraph">
<p>我们已经在 <a href="#token-normalization">归一化词元</a> 看过自由变音匹配， <a href="#stemming">将单词还原为词根</a> 中的词干， <a href="#synonyms">同义词</a> 中的同义词，
但所有这些方法假定单词拼写正确，或者每个单词拼写只有唯一的方法。</p>
</div>
<div class="paragraph">
<p>Fuzzy matching 允许查询时匹配错误拼写的单词，而语音语汇单元过滤器可以在索引时用来进行 <em>近似读音</em> 匹配。</p>
</div>
<div class="sect2">
<h3 id="fuzziness">模糊性</h3>
<div class="paragraph">
<p><em>模糊匹配</em> 对待 “模糊” 相似的两个词似乎是同一个词。首先，我们需要对我们所说的  <em>模糊性</em> 进行定义。</p>
</div>
<div class="paragraph">
<p>在1965年，Vladimir Levenshtein 开发出了 <a href="http://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a>，
用来度量从一个单词转换到另一个单词需要多少次单字符编辑。他提出了三种类型的单字符编辑：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>一个字符 <em>替换</em> 另一个字符： _f_ox &#8594; _b_ox</p>
</li>
<li>
<p><em>插入</em> 一个新的字符：sic &#8594; sic_k_</p>
</li>
<li>
<p><em>删除</em> 一个字符：b_l_ack &#8594; back</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><a href="http://en.wikipedia.org/wiki/Frederick_J._Damerau">Frederick Damerau</a>
后来在这些操作基础上做了一个扩展：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>相邻两个字符的 <em>换位</em> ： _st_ar &#8594; _ts_ar</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>举个例子，将单词 <code>bieber</code> 转换成 <code>beaver</code> 需要下面几个步骤：</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>把 <code>b</code> 替换成 <code>v</code> ：bie_b_er &#8594; bie_v_er</p>
</li>
<li>
<p>把 <code>i</code> 替换成 <code>a</code> ：b_i_ever &#8594; b_a_ ever</p>
</li>
<li>
<p>把 <code>e</code> 和 <code>a</code> 进行换位：b_ae_ver &#8594; b_ea_ver</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>这三个步骤表示 <a href="https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance">Damerau-Levenshtein edit distance</a> 编辑距离为 3 。</p>
</div>
<div class="paragraph">
<p>显然，从 <code>beaver</code> 转换成 <code>bieber</code> 是一个很长的过程&#x2014;他们相距甚远而不能视为一个简单的拼写错误。
Damerau 发现 80% 的拼写错误编辑距离为 1 。换句话说， 80% 的拼写错误可以对原始字符串用 <em>单次编辑</em> 进行修正。</p>
</div>
<div class="paragraph">
<p>Elasticsearch 指定了 <code>fuzziness</code> 参数支持对最大编辑距离的配置，默认为 ２ 。</p>
</div>
<div class="paragraph">
<p>当然，单次编辑对字符串的影响取决于字符串的长度。对单词 <code>hat</code> 两次编辑能够产生  <code>mad</code> ，
所以对一个只有 3 个字符长度的字符串允许两次编辑显然太多了。
 <code>fuzziness</code> 参数可以被设置为 <code>AUTO</code> ，这将导致以下的最大编辑距离：</p>
</div>
<div class="ulist">
<ul>
<li>
<p>字符串只有 1 到 2 个字符时是 <code>0</code></p>
</li>
<li>
<p>字符串有 3 、 4 或者 5 个字符时是 <code>1</code></p>
</li>
<li>
<p>字符串大于 5 个字符时是 <code>2</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>当然，你可能会发现编辑距离 <code>2</code> 仍然是太多了，返回的结果似乎并不相关。
把最大 <code>fuzziness</code> 设置为 <code>1</code> ，你可以得到更好的结果和更好的性能。</p>
</div>
</div>
<div class="sect2">
<h3 id="fuzzy-query">模糊查询</h3>
<div class="paragraph">
<p>{ref}/query-dsl-fuzzy-query.html[<code>fuzzy</code> 查询]是 <code>term</code> 查询的模糊等价。
也许你很少直接使用它，但是理解它是如何工作的，可以帮助你在更高级别的 <code>match</code> 查询中使用模糊性。</p>
</div>
<div class="paragraph">
<p>为了解它是如何运作的，我们首先索引一些文档：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">POST /my_index/my_type/_bulk
{ "index": { "_id": 1 }}
{ "text": "Surprise me!"}
{ "index": { "_id": 2 }}
{ "text": "That was surprising."}
{ "index": { "_id": 3 }}
{ "text": "I wasn't surprised."}</code></pre>
</div>
</div>
<div class="paragraph">
<p>现在我们可以为词 <code>surprize</code> 运行一个 <code>fuzzy</code> 查询：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/my_type/_search
{
  "query": {
    "fuzzy": {
      "text": "surprize"
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>fuzzy</code> 查询是一个词项级别的查询，所以它不做任何分析。它通过某个词项以及指定的 <code>fuzziness</code> 查找到词典中所有的词项。
<code>fuzziness</code> 默认设置为 <code>AUTO</code> 。</p>
</div>
<div class="paragraph">
<p>在我们的例子中， <code>surprise</code> 比较 <code>surprise</code> 和 <code>surprised</code> 都在编辑距离 2 以内，
所以文档 1 和 3 匹配。通过以下查询，我们可以减少匹配度到仅匹配 <code>surprise</code> ：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/my_type/_search
{
  "query": {
    "fuzzy": {
      "text": {
        "value": "surprize",
        "fuzziness": 1
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_提高性能">提高性能</h4>
<div class="paragraph">
<p><code>fuzzy</code> 查询的工作原理是给定原始词项及构造一个 <em>编辑自动机</em>&#x2014;
像表示所有原始字符串指定编辑距离的字符串的一个大图表。</p>
</div>
<div class="paragraph">
<p>然后模糊查询使用这个自动机依次高效遍历词典中的所有词项以确定是否匹配。
一旦收集了词典中存在的所有匹配项，就可以计算匹配文档列表。</p>
</div>
<div class="paragraph">
<p>当然，根据存储在索引中的数据类型，一个编辑距离 2 的模糊查询能够匹配一个非常大数量的词项同时执行效率会非常糟糕。
下面两个参数可以用来限制对性能的影响：</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>prefix_length</code></dt>
<dd>
<p>不能被 <code>`模糊化'' 的初始字符数。
大部分的拼写错误发生在词的结尾，而不是词的开始。
例如通过将 `prefix_length</code> 设置为 <code>3</code> ，你可能够显著降低匹配的词项数量。</p>
</dd>
<dt class="hdlist1"><code>max_expansions</code></dt>
<dd>
<p>如果一个模糊查询扩展了三个或四个模糊选项， 这些新的模糊选项也许是有意义的。如
果它产生 1000 个模糊选项，那么就基本没有意义了。
设置 <code>max_expansions</code> 用来限制将产生的模糊选项的总数量。模糊查询将收集匹配词项直到达到 <code>max_expansions</code> 的限制。</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect2">
<h3 id="fuzzy-match-query">模糊匹配查询</h3>
<div class="paragraph">
<p><code>match</code> 查询支持开箱即用的模糊匹配：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/my_type/_search
{
  "query": {
    "match": {
      "text": {
        "query":     "SURPRIZE ME!",
        "fuzziness": "AUTO",
        "operator":  "and"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>查询字符串首先进行分析，会产生词项 <code>[surprize, me]</code> ，并且每个词项根据指定的 <code>fuzziness</code> 进行模糊化。</p>
</div>
<div class="paragraph">
<p>同样， <code>multi_match</code> 查询也支持 <code>fuzziness</code> ，但只有当执行查询时类型是 <code>best_fields</code> 或者 <code>most_fields</code> ：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/my_type/_search
{
  "query": {
    "multi_match": {
      "fields":  [ "text", "title" ],
      "query":     "SURPRIZE ME!",
      "fuzziness": "AUTO"
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>match</code> 和 <code>multi_match</code> 查询都支持 <code>prefix_length</code> 和 <code>max_expansions</code> 参数。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
模糊性（Fuzziness）只能在 <code>match</code> and <code>multi_match</code> 查询中使用。不能使用在短语匹配、常用词项或 <code>cross_fields</code> 匹配。
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="fuzzy-scoring">模糊性评分</h3>
<div class="paragraph">
<p>用户喜欢模糊查询。他们认为这种查询会魔法般的找到正确拼写组合。

很遗憾，实际效果平平。</p>
</div>
<div class="paragraph">
<p>假设我们有1000个文档包含 <code>Schwarzenegger</code> ，只是一个文档的出现拼写错误 <code>Schwarzeneger</code> 。
根据 <a href="#tfidf">term frequency/inverse document frequency</a> 理论，这个拼写错误文档比拼写正确的相关度更高，因为错误拼写出现在更少的文档中！</p>
</div>
<div class="paragraph">
<p>换句话说，如果我们对待模糊匹配类似其他匹配方法，我们将偏爱错误的拼写超过了正确的拼写，这会让用户抓狂。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
模糊匹配不应用于参与评分&#8212;&#8203;只能在有拼写错误时扩大匹配项的范围。
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>默认情况下， <code>match</code> 查询给定所有的模糊匹配的恒定评分为1。这可以满足在结果列表的末尾添加潜在的匹配记录，并且没有干扰非模糊查询的相关性评分。</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph">
<p>在模糊查询最初出现时很少能单独使用。他们更好的作为一个 <code>bigger</code> 场景的部分功能特性，如 <em>search-as-you-type</em>
{ref}/search-suggesters-completion.html[<code>完成</code> 建议]或
<em>did-you-mean</em> {ref}/search-suggesters-phrase.html[<code>短语</code> 建议]。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="phonetic-matching">语音匹配</h3>
<div class="paragraph">
<p>最后，在尝试任何其他匹配方法都无效后，我们可以求助于搜索发音相似的词，即使他们的拼写不同。</p>
</div>
<div class="paragraph">
<p>有一些用于将词转换成语音标识的算法。
 <a href="http://en.wikipedia.org/wiki/Soundex">Soundex</a> 算法是这些算法的鼻祖，
而且大多数语音算法是 Soundex 的改进或者专业版本，例如 <a href="http://en.wikipedia.org/wiki/Metaphone">Metaphone</a>
和 <a href="http://en.wikipedia.org/wiki/Metaphone#Double_Metaphone">Double Metaphone</a> （扩展了除英语以外的其他语言的语音匹配），
<a href="http://en.wikipedia.org/wiki/Caverphone">Caverphone</a> 算法匹配了新西兰的名称，
<a href="https://en.wikipedia.org/wiki/Daitch–Mokotoff_Soundex#Beider.E2.80.93Morse_Phonetic_Name_Matching_Algorithm">Beider-Morse</a> 算法吸收了 Soundex 算法为了更好的匹配德语和依地语名称，
<a href="http://de.wikipedia.org/wiki/K%C3%B6lner_Phonetik">Kölner Phonetik</a> 为了更好的处理德语词汇。</p>
</div>
<div class="paragraph">
<p>值得一提的是，语音算法是相当简陋的，他们设计初衷针对的语言通常是英语或德语。这限制了他们的实用性。
不过，为了某些明确的目标，并与其他技术相结合，语音匹配能够作为一个有用的工具。</p>
</div>
<div class="paragraph">
<p>首先，你需要从
<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-phonetic.html" class="bare">https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-phonetic.html</a> 获取语音分析插件并在集群的每个节点安装，
然后重启每个节点。</p>
</div>
<div class="paragraph">
<p>然后，您可以创建一个使用语音语汇单元过滤器的自定义分析器，并尝试下面的方法：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index
{
  "settings": {
    "analysis": {
      "filter": {
        "dbl_metaphone": { <b class="conum">(1)</b>
          "type":    "phonetic",
          "encoder": "double_metaphone"
        }
      },
      "analyzer": {
        "dbl_metaphone": {
          "tokenizer": "standard",
          "filter":    "dbl_metaphone" <b class="conum">(2)</b>
        }
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>首先，配置一个自定义 <code>phonetic</code> 语汇单元过滤器并使用 <code>double_metaphone</code> 编码器。</p>
</li>
<li>
<p>然后在自定义分析器中使用自定义语汇单元过滤器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>现在我们可以通过 <code>analyze</code> API 来进行测试：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/_analyze?analyzer=dbl_metaphone
Smith Smythe</code></pre>
</div>
</div>
<div class="paragraph">
<p>每个  <code>Smith</code> 和 <code>Smythe</code> 在同一位置产生两个语汇单元： <code>SM0</code> 和 <code>XMT</code> 。
通过分析器播放 <code>John</code> ， <code>Jon</code> 和 <code>Johnnie</code> 将产生两个语汇单元   <code>JN</code> 和 <code>AN</code> ，而 <code>Jonathon</code> 产生语汇单元 <code>JN0N</code> 和 <code>ANTN</code> 。</p>
</div>
<div class="paragraph">
<p>语音分析器可以像任何其他分析器一样使用。 首先映射一个字段来使用它，然后索引一些数据：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">PUT /my_index/_mapping/my_type
{
  "properties": {
    "name": {
      "type": "string",
      "fields": {
        "phonetic": { <b class="conum">(1)</b>
          "type":     "string",
          "analyzer": "dbl_metaphone"
        }
      }
    }
  }
}

PUT /my_index/my_type/1
{
  "name": "John Smith"
}

PUT /my_index/my_type/2
{
  "name": "Jonnie Smythe"
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p><code>name.phonetic</code> 字段使用自定义 <code>dbl_metaphone</code> 分析器。</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>可以使用 <code>match</code> 查询来进行搜索：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">GET /my_index/my_type/_search
{
  "query": {
    "match": {
      "name.phonetic": {
        "query": "Jahnnie Smeeth",
        "operator": "and"
      }
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个查询返回全部两个文档，演示了如何进行简陋的语音匹配。
 用语音算法计算评分是没有价值的。
语音匹配的目的不是为了提高精度，而是要提高召回率&#8212;&#8203;以扩展足够的范围来捕获可能匹配的文档。</p>
</div>
<div class="paragraph">
<p>通常更有意义的使用语音算法是在检索到结果后，由另一台计算机进行消费和后续处理，而不是由人类用户直接使用。</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2018-05-17 22:26:24 CST
</div>
</div>
</body>
</html>